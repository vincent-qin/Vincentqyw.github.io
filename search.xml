<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[深度学习在深度(视差)估计中的应用]]></title>
      <url>%2F2017%2F12%2F06%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E6%B7%B1%E5%BA%A6(%E8%A7%86%E5%B7%AE)%E4%BC%B0%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[本文对KITTI stereo 2015 datasets 冠军之作Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching进行简要解读。 前言目前深度学习发展的如火如荼，利用CNN可以将图像对的匹配问题看成一个学习问题。但是如何能够得到高质量的深度图像仍然是一个普世的问题。本文作者提出了一种新型的层叠式（cascade）CNN结构（CRL:Cascade Residual Learning）去估计深度信息。深度估计的过程大致可以分成两个步骤： 在现有的DispNet的基础上添加几个上卷积模块，目的是为了得到full resolution的初始的深度信息，同时能够学习到更多的细节信息； 第二步是对第一步中学习到的深度信息进行校准（rectify）；这一步利用到了第一步得到的多尺度的深度信息，然后并非是直接学习到优化后的深度信息，而是学习了每个尺度下的深度残差，然后结合第一步中得到的多尺度深度信息合成最终的深度图（这里有点类似于何凯明的residual的思想： It is easier to learn the residual than to learn the disparity directly）。 网络结构下面详细的介绍下这个网络的结构: 可以很清楚地在上图中看到这两个不同的阶段。对于第一个阶段，类似于文献[1]中提到的DispNetC结构（C是correlation层的意思），本文作者同样采取了沙漏形的网络结构。但是DispNetC网络的输出图像的分辨率只有原始尺寸的一半！CRL中的DispFulNet在DispNetC的基础上，在最后的两个卷积层增加了添加了上卷积模块，然后再串联左图；通过再次添加一个额外的卷积层，可以使得网络输出为全分辨率（和左右图大小一致）。注意：每个尺度（共6个尺度）上的临时输出与其对应的ground truth之间计算$l_1$损失。总结一下就是，这个DispFulNet学习了这样一个网络：通过输入一对图片$I_L$和$I_R$，学习到了视差$d_1$，使得： \tilde{I}_L(x,y)=I_R(x+d_1(x,y),y)上式中的$\tilde{I}_L$就是把右图根据视差移动后的结果，我们的目标就是$\tilde{I}_L$越来越接近$I_L$。 接下来就是第二阶段，将$I_L$,$I_R$,$\tilde{I}_L$,$d_1$以及$e_L=|I_L-\tilde{I}_L|$串联起来[2]作为dispResNet的输入。此优化网络最后学到的是多尺度的残差$ \{r_2^{(s)} \} _s^S$，其中s=0时表示全尺度残差。最后与DispFulNet输出的多尺度深度图$\{d_1^{(s)}\}_s^S$做和运算得到最后优化后的深度$\{d_2^{(s)}\}_s^S$： d_2^{(s)}=d_1^{(s)}+r_2^{(s)},0 \leq s \leq S于是$d_2^{(0)}$就是最后的全尺度输出。 实验结果以下是对其结果展示： 参考文献[1]. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040–4048, 2016.[2]. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2462–2470, 2017.[3]. KITTI: Stereo Evaluation 2015[4]. code: Cascade Residual Learning (CRL)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matlab Deep Learning学习笔记]]></title>
      <url>%2F2017%2F11%2F19%2FMatlab-Deep-Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[最近对深度学习尤其着迷，是时候用万能的Matlab去践行我的DL学习之路了。之所以用Matlab，是因为Matlab真的太强大了！自从大学开始我就一直用这个神奇的软件，算是最熟悉的编程工具。加上最近mathworks公司一大波大佬的不懈努力，在今年下半年发行的R2017b版本中又加入了诸多新颖的特性，尤其在DL方面，可以发现：仅仅几条简单的代码，就能够实现复杂的功能。基于以上，我在本文列举了几个在Matlab上学习Deep Learning的例子：1. 手写字符识别；2. 搭建网络对CIFAR10分类；3.搭建一个Resnet。务必保证主机已经安装Matlab 2017a及以上。 手写字符识别利用CNN做数字分类实验。 接下来的实验会阐明如何进行： 加载图像数据 设计网络结构 设置网络训练参数 训练网络 预测新数据的类别 加载图像数据12345678910111213digitDatasetPath = fullfile(matlabroot,'toolbox','nnet','nndemos',... 'nndatasets','DigitDataset');% imageDatastore函数 能够通过文件夹名自动地把数据存储成ImageDatastore 对象digitData = imageDatastore(digitDatasetPath,... 'IncludeSubfolders',true,'LabelSource','foldernames');% Display some of the images in the datastore.figure;perm = randperm(10000,25);for i = 1:25 subplot(5,5,i); imshow(digitData.Files&#123;perm(i)&#125;);end 以下是手写字符的部分数据： 创建训练集与验证集123trainNumFiles = 750;[trainDigitData,valDigitData] = splitEachLabel(digitData,750,'randomize'); % 每类有1000个，选择其中的750类作为训练集，剩下的作为验证集；此处750可以换成一个比例：75% 注意Matlab里面支持的层的类型，包括：CLICK THIS LINK。如下所示： Epoch Iteration Layer Type Function Image input layer imageInputLayer Sequence input layer sequenceInputLayer 2-D convolutional layer convolution2dLayer 2-D transposed convolutional layer transposedConv2dLayer Fully connected layer fullyConnectedLayer Long short-term memory (LSTM) layer LSTMLayer Rectified linear unit (ReLU) layer reluLayer Leaky rectified linear unit (ReLU) layer leakyReluLayer Clipped rectified linear unit (ReLU) layer clippedReluLayer Batch normalization layer batchNormalizationLayer Channel-wise local response normalization (LRN) layer crossChannelNormalizationLayer Dropout layer dropoutLayer Addition layer additionLayer Depth concatenation layer depthConcatenationLayer Average pooling layer averagePooling2dLayer Max pooling layer maxPooling2dLayer Max unpooling layer maxUnpooling2dLayer Softmax layer softmaxLayer Classification layer classificationLayer Regression layer regressionLayer 创建自己的网络结构123456789101112131415161718192021222324%% Define Network Architecture% Define the convolutional neural network architecture.layers = [ imageInputLayer([28 28 1]) convolution2dLayer(3,16,'Padding',1) batchNormalizationLayer() reluLayer() maxPooling2dLayer(2,'Stride',2) convolution2dLayer(3,32,'Padding',1) batchNormalizationLayer() reluLayer() maxPooling2dLayer(2,'Stride',2) convolution2dLayer(3,64,'Padding',1) batchNormalizationLayer() reluLayer() fullyConnectedLayer(10) softmaxLayer() classificationLayer() ]; 以下就是该网络结构及参数设置： 123456789101112131415 1 '' Image Input 28x28x1 images with 'zerocenter' normalization 2 '' Convolution 16 3x3 convolutions with stride [1 1] and padding [1 1 1 1] 3 '' Batch Normalization Batch normalization 4 '' ReLU ReLU 5 '' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0] 6 '' Convolution 32 3x3 convolutions with stride [1 1] and padding [1 1 1 1] 7 '' Batch Normalization Batch normalization 8 '' ReLU ReLU 9 '' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]10 '' Convolution 64 3x3 convolutions with stride [1 1] and padding [1 1 1 1]11 '' Batch Normalization Batch normalization12 '' ReLU ReLU13 '' Fully Connected 10 fully connected layer14 '' Softmax softmax15 '' Classification Output crossentropyex 网络训练参数设计123456 options = trainingOptions('sgdm',...'MaxEpochs',3, ... % 训练最大轮回'ValidationData',valDigitData,... % 验证集'ValidationFrequency',30,...'Verbose',false,...'Plots','training-progress'); 开始训练1net = trainNetwork(trainDigitData,layers,options); 测试新的数据123predictedLabels = classify(net,valDigitData);valLabels = valDigitData.Labels;accuracy = sum(predictedLabels == valLabels)/numel(valLabels) 查看某层参数例如查看第2层的weight参数，输入以下命令：123456montage(imresize(mat2gray(net.Layers(2).Weights),[128 128]));set(gcf,'color',[1 1 1]); frame=getframe(gcf); % get the frameimage=frame.cdata;[image,map] = rgb2ind(image,256); imwrite(image,map,'weight-layer2.png'); 图像如下所示： 再看一下第10层的参数：12345678910111213141516171819[~,~,iter,~]=size(net.Layers(10).Weights);name='weight.gif';dt=0.4;for i=1:iter montage(imresize(mat2gray(net.Layers(10).Weights(:,:,i,:)),[128 128])); set(gcf,'color',[1 1 1]); %变白 title(['Layer(10), Channel: ',num2str(i)]); axis normal truesize %Creat GIF frame(i)=getframe(gcf); % get the frame image=frame(i).cdata; [image,map] = rgb2ind(image,256); if i==1 imwrite(image,map,name,'gif'); else imwrite(image,map,name,'gif','WriteMode','append','DelayTime',dt); endend 搭建网络对CIFAR10分类CIFAR10和CIFAR100是80 million tiny images的子集，是由Geoffrey Hinton的弟子们Alex Krizhevsky和Vinod Nair共同采集。 CIFAR10CIFAR10由60000张32*32的彩色图像组成，一种分成10类，平均每类图像6000张。共有50000张训练图像，10000张测试图像。这个数据集被分成了5个分支，其中每个分支10000张。测试集包含每类中随机选择的1000张图像。训练集就是剩下的那些图像。对于每个分支的数据的大小是：10000*3072；其中3072=32*32*3。数据以行优先的顺序存储，所以前1024个数据是r通道的数据，接下来的1024个数据是g通道的数据，最后1024个数据是b通道的。假如原始的数据是data，我们想要将其重新排列成我们需要的数据。首先对其进行转置，然后再用reshape函数对图像重组（可选：最后将图像前两维互换（转置），之所以这么做，可以更好的可视化）。 123XBatch = data';XBatch = reshape(XBatch, 32,32,3,[]);XBatch = permute(XBatch, [2 1 3 4]); 以下是cifar10的部分数据。共有10类，包括：airplane，automobile，bird，cat，deer，dog，frog，horse，ship，truck。 Just run it接下来我们就开始运行以下代码，来训练我们的网络。闲话少说，我把代码放在了Github，欢迎$star$。 1234567891011121314151 'imageinput' Image Input 28x28x1 images with 'zerocenter' normalization2 'conv_1' Convolution 16 3x3x1 convolutions with stride [1 1] and padding [1 1 1 1]3 'batchnorm_1' Batch Normalization Batch normalization with 16 channels4 'relu_1' ReLU ReLU5 'maxpool_1' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]6 'conv_2' Convolution 32 3x3x16 convolutions with stride [1 1] and padding [1 1 1 1]7 'batchnorm_2' Batch Normalization Batch normalization with 32 channels8 'relu_2' ReLU ReLU9 'maxpool_2' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]10 'conv_3' Convolution 64 3x3x32 convolutions with stride [1 1] and padding [1 1 1 1]11 'batchnorm_3' Batch Normalization Batch normalization with 64 channels12 'relu_3' ReLU ReLU13 'fc' Fully Connected 10 fully connected layer14 'softmax' Softmax softmax15 'classoutput' Classification Output crossentropyex with '0', '1', and 8 other classes 以下是训练过程输出： Epoch Iteration Time Elapsed (seconds) Mini-batch Loss Mini-batch Accuracy Base Learning Rate 1 1 0.06 2.3026 8.59% 0.0020 1 50 1.27 2.3026 14.06% 0.0020 1 100 2.52 2.3024 7.81% 0.0020 1 150 3.73 2.2999 20.31% 0.0020 1 200 5.01 2.2740 15.63% 0.0020 1 250 6.28 2.1194 21.09% 0.0020 1 300 7.58 1.9100 23.44% 0.0020 1 350 8.86 1.8892 28.13% 0.0020 2 400 10.08 1.7490 29.69% 0.0020 2 450 11.32 1.8377 31.25% 0.0020 2 500 12.57 1.6073 39.84% 0.0020 … … … … … … 20 7650 407.74 0.2858 93.75% 2.00e-05 20 7700 409.06 0.3127 89.84% 2.00e-05 20 7750 410.38 0.3254 87.50% 2.00e-05 20 7800 411.64 0.2456 92.19% 2.00e-05 最后测试我们的模型的性能，accuracy=76%左右。但是训练时，我们的batch-accuracy已经达到了90%以上，说明我们的模型过拟合了。显然这不是我们想要的结果，进一步的调参将会在此补充。 可视化某层的参数123456789101112131415% Extract the first convolutional layer weightsw = cifar10Net.Layers(2).Weights;% rescale and resize the weights for better visualizationw = mat2gray(w);w = imresize(w, [100 100]);figuremontage(w)name='cifar10-weight-layer2';set(gcf,'color',[1 1 1]);frame=getframe(gcf); % get the frameimage=frame.cdata;[image,map] = rgb2ind(image,256); imwrite(image,map,[name,'.png']); 搭建一个Resnet接下来，为了验证下这个DL工具包的强大之处，我打算纯手工建一个Resnet。为方便起见，我搭了一个Resnet34（更深的网络敬请期待吧）。这里是它的prototxt，我们可以用网络可视化工具进行查看resnet34的结构。以下是Resnet34的一部分（太长了没有截下全部视图）。 定义每一层与连接层以从pool1到res2a为例子建立网络。 12345678910111213141516layers_example=[ % pool1 - res2a maxPooling2dLayer(3, 'Stride', 2,'Name','pool1'); % branch2a convolution2dLayer(3,64,'Stride', 1,'Padding', 1,'Name','res2a_branch2a') batchNormalizationLayer('Name','bn2a_branch2a') reluLayer('Name','res2a_branch2a_relu') % branch2b convolution2dLayer(3,64,'Stride', 1,'Padding', 1,'Name','res2a_branch2b') batchNormalizationLayer('Name','bn2a_branch2b') % add together additionLayer(2,'Name','res2a') reluLayer('Name','res2a_relu')]; 上述过程仅仅完成了网络的一个小分支，记下来要完成res2a_branch1这部分的连接。这时候要用到DAG的一些方法。通过添加新层同时建立新的连接即可，方式如下。 1234567891011121314lgraph = layerGraph(layers_example);figureplot(lgraph)%% add some connections (shortcut)layers_2a=[ convolution2dLayer(1,64,'Stride', 1,'Padding', 0,'Name','res2a_branch1') batchNormalizationLayer('Name','bn2a_branch1')];lgraph = addLayers(lgraph,layers_2a);lgraph = connectLayers(lgraph,'pool1','res2a_branch1');lgraph = connectLayers(lgraph,'bn2a_branch1','res2a/in2');% show netplot(lgraph) 其他部分的构建同上，经过一系列重复的工作，我们可以构建出这个不太深的Resnet34，全部代码见我的Github。 一些基本问题 参数的基本格式 Height \times Width \times (\#Channels) \times (\#Filters) SGD是什么？可以参见好友写的一篇博文。 什么是epoch？模型训练的时候一般采用stochastic gradient descent（SGD），一次迭代选取一个batch进行update。一个epoch的意思就是迭代次数*batch的数目 和训练数据的个数一样，就是一个epoch。 为什么要是用BN？Batch normalization layers normalize the activations and gradients propagating through a network, making network training an easier optimization problem. Use batch normalization layers between convolutional layers and nonlinearities, such as ReLU layers, to speed up network training and reduce the sensitivity to network initialization. RELU的作用？Max-Pooling Layer Convolutional layers (with activation functions) are sometimes followed by a down-sampling operation that reduces the spatial size of the feature map and removes redundant spatial information. Down-sampling makes it possible to increase the number of filters in deeper convolutional layers without increasing the required amount of computation per layer. One way of down-sampling is using a max pooling. The max pooling layer returns the maximum values of rectangular regions of inputs. add more Resnet中scale层是如何定义的？有什么用途？ Resnet中为何残差$F(x)$比$H(x)$好学？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CNN框架(CNN Architectures)]]></title>
      <url>%2F2017%2F11%2F07%2FCNN%E6%A1%86%E6%9E%B6-CNN-Architectures%2F</url>
      <content type="text"><![CDATA[本文来自于CS231N（2017 Spring），将介绍几种较为常见的CNN结构。以下网络均是ImageNet比赛的冠军之作，我们将从网络结构，参数规模，运算量等来描述各个网络的特点。 AlexNet VGG GoogLeNet ResNet 后续将补充以下几种网络： NiN(Network in Network) wide ResNet ResNeXT stochastic Depth DenseNet FractalNet SqueezeNet 以下是正文。 AlexNet网络结构网络的输入大小为：227*227*3，每一层的结构以及参数设置如下： Layer Type #Filters Stride Pading OUTPUT SIZE Parameters CONV1 #96 @11*11 4 0 55*55*96 11*11*3*96 MAXPOOL1 3*3 2 0 27*27*96 0 NORM1 27*27*96 55*55*96 CONV1 #256 @5*5 1 2 27*27*256 55*55*96 MAXPOOL2 3*3 2 0 13*13*256 55*55*96 NORM2 13*13*256 55*55*96 CONV3 #384 @3*3 1 1 13*13*384 55*55*96 CONV4 #384 @3*3 1 1 13*13*384 55*55*96 CONV5 #256 @3*3 1 1 13*13*256 55*55*96 MAXPOOL3 3*3 2 0 6*6*256 55*55*96 FC6 4096 55*55*96 FC7 4096 55*55*96 FC8 1000 55*55*96 The size of output image is $\frac{N-Conv+2\times Pading}{stride}+1$ 后续将使用Matlab DL 工具包补充Alexnet实验… VGGNetThe winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014. 网络结构small filters, deeper networks。将原来8层的AlexNet扩展到了16&amp;19层。卷积层的大小仅仅有3*3，stride=1，pad=1；池化层仅仅有stride=2的2*2的MAXPOOL。以下是其与AlexNet的结构对比图。 更加具体的，VGG16的网络的参数个数以及内存消耗如下： Q：为何采用更小的CONV？A：几个3*3的CONV叠加后的接受域和一个7*7大小的CONV的接受域一致，但是与此同时，网络层数变深，引入了更多的非线性，参数数量更少。（Stack of three 3x3 conv (stride 1) layers has same effective receptive field as one 7x7 conv layer，But deeper, more non-linearities. And fewer parameters: $3\times3^2C^2$ vs. $7^2C^2$ for C channels per layer） 更多细节 ILSVRC’14 2nd in classification, 1st in localization Similar training procedure as Krizhevsky 2012 No Local Response Normalisation (LRN) Use VGG16 or VGG19 (VGG19 only slightly better, more memory) Use ensembles for best results FC7 features generalize well to other tasks GoogLeNet论文地址：https://arxiv.org/pdf/1409.4842.pdf代码地址：NULLDeeper networks, with computational efficiency。GoogLeNet是ILSVRC’14的图像分类冠军网络，它加入了Inception模块，并且去除了全连接层，大大减少了参数的个数。 22 layers (with weights) Efficient “Inception” module No FC layers Only 5 million parameters! 12x less than AlexNet ILSVRC’14 classification winner (6.7% top 5 error) “Inception module”精心设计了一个局部网络模块，并且将这些模块叠加构成GoolgeNet。这种经过精心设计的模块就是Inception。（design a good local network topology (network within a network) and then stack these modules on top of each other）。Inception包含几个接受域不同的CONV核（1*1，3*3，5*5）以及池化操作（3*3）；最终将这些操作后的输出在depth方向串联。以下是两种两种不同的实现方式，左图时原始的inception模块，右图是改进版的inception模块。对于naive inception而言，它面临这运算量巨大的问题。由于池化层的输出会保留原始输入的depth，所以经过CONV&amp;MAXPOOL过后的输出的feature map势必比原始输入的depth更深。那么如何去解决以上问题呢，一个通常的方式就是降维。我们在每个CONV前加上1*1的CONV（“bottleneck” layers）来减少feature map的维度。所谓的1*1CONV就是在保持输入的空间分辨率不变的情况下来减小depth维度，即通过将不同depth上的feature map进行组合，从而将输入的feature map映射到更低的depth维度上。经过以上操作就可以将运算的操作次数大大降低。 于是GoogLeNet的全貌如下： ResNet利用残差连接成的超级深网络。这里有一个何凯明在ICML2016的Tutorial，内容比较详细。ICML 2016 Tutorial on Deep Residual Networks代码在这里Code: deep-residual-networks 概况 152-layer model for ImageNet ILSVRC’ 15 classification winner (3.57% top 5 error) Swept all classification and detection competitions in ILSVRC’ 15 and COCO’ 15! 深度增加带来的问题从上图可以发现，当网络层数增加时，训练误差和测试误差都有所下降。这并不符合以往的经验，我们会想，既然网络层数增加了，那么模型参数势必增多，此时会造成过拟合。然而过拟合的表现是：训练误差减小，测试误差增大。但是事实和分析并不吻合。何凯明认为：The problem is an optimization problem, deeper models are harder to optimize。这是一个优化问题，更深的网络更难优化。并且，更深的网络应该至少比浅层网络不差，这是因为我们可以通过拷贝浅层网络+identity mapping（恒等映射）来构造一个更深的网络，这个结构化的方案表明深层网络可以达到和浅层网络一致的性能。 解决方案Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.作者假设：相较于最优化最初的无参照映射（残差函数以输入x作为参照），最优化残差映射是更容易的。利用网络去拟合残差$F(x)$，并非直接拟合$H(x)$。 整个ResNet框架 Stack residual blocks Every residual block has two 3x3 conv layers Periodically, double # of filters and downsample spatially using stride 2 (/2 in each dimension) Additional conv layer at the beginning No FC layers at the end (only FC 1000 to output classes) 对于ImageNet比赛而言，ResNet设置的网络深度有34、50、101以及152层。对于层数较多的网络，利用“bottleneck”（类似于GoogLeNet的1*1卷积操作）来提高效率。 总结论文An Analysis of Deep Neural Network Models for Practical Applications 比较了2016年以来的一些神经网络的规模、运算量、能耗以及精度等项目。可以从上图总结出以下几点： GoogLeNet: most efficient VGG: Highest memory, most operations AlexNet: Smaller compute, still memory heavy, lower accuracy ResNet: Moderate efficiency depending on model, highest accuracy 其他网络变体后续补充。 疑问 ResNet为何能够使网络层数更深，应如何正确理解残差网络？He是受何启发从而发明了这种结构？ more questions will be added… 参考文献 DeepLearning.net Reading List ImageNet Classification with Deep Convolutional Neural Networks 为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题 An Analysis of Deep Neural Network Models for Practical Applications CS231n: Convolutional Neural Networks for Visual Recognition Densely Connected Convolutional Networks Deep Residual Networks (Deep Learning Gets Way Deeper)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[理解LSTM网络【译】]]></title>
      <url>%2F2017%2F10%2F23%2F%E7%90%86%E8%A7%A3LSTM%E7%BD%91%E7%BB%9C%E3%80%90%E8%AF%91%E3%80%91%2F</url>
      <content type="text"><![CDATA[本文是我对大神Christopher Olah的Understanding LSTM Networks的译文。 循环神经网络（Recurrent Neural Networks）人们并非每时每刻都在大脑一片空白时开始思考。当我们读这篇文章的时候，我们会根据之前学习的知识来理解当前你在阅读的内容。我们不是把原来的知识丢的一干二净来重新学习，我们的记忆有一定的持久性。传统的神经网络做不到这些，这是它的一大缺陷。比如说，可以想象有这样一种情况，我们想知道一部电影的每一帧画面正在发生什么。使用传统的神经网络很难通过理解电影之前的画面来推断以后将要发生的事件。（传统的神经网络不能处理带有时序的样本）循环神经网络（Recurrent Neural Networks）尝试解决了以上问题。这种网络是一种带有循环结构的网络，可以使得信息得以持久保持。 上图是一个RNN模块，$A$读取输入$x_i$，同时输出$h_t$，$A$这个循环允许信息从网络的当前步骤传递到下一步骤。上述过程把RNN的过程讲的有些神秘感。但是，如果我们仔细想想，这也不比一个正常的神经网络难以理解。一个RNN可以看成是多个相同网络的拷贝，每一个拷贝都会向后续网络传递信息，下图我们把RNN展开。这种链式的特性揭示了RNN与序列和列表有关，RNN是对这些数据最自然的表达。RNN目前已经被广泛使用！在过去的几年间，RNN在很多领域都有着出色的表现：语音识别，语言建模，翻译，图像描述…推荐大家阅读大神Andrej Karpathy的博文 The Unreasonable Effectiveness of Recurrent Neural Networks，来领略下RNN的诸多应用，这简直不能太棒！以上的成功案例离不开使用长短期记忆（LSTM: Long Short Term Memory）网络，这是一种特殊的RNN网络并且能够应对更多任务，相比于标准的RNN网络，它具有更为出色的表现。几乎所有的RNN都是基于LSTM来实现的，接下来我们讨论下LSTM的奥义。 长期依赖（Long-Term Dependencies）问题RNNs的要求之一就是它能够连接之前的信息到当前的任务之上，例如利用之前的视频帧来理解当前帧的内容。如果RNNs能够做到这一点的话，它将会超级有用。但是它真的可以吗？看情况而定。有时，我们仅仅需要离当前任务最近的几个任务信息。例如，我们有一个语言模型，它的目标是根据当前已有的词语去预测接下来将要出现的词语。如果我们尝试去预测“the clouds are in the sky”中的最后一个单词，我们不需要任何更多的语料，很明显最后一个单词将会是“sky”。在这种情况下，相关信息和当前需要预测词的位置的间隔是非常小的，这时RNNs可以去利用过去的信息。但是也有一种情况是，我们需要更多的信息才能够做预测。例如我们的语言模型需要预测下面句子的最后一个单词“I grew up in France… I speak fluent French.”。从相邻近的几个单词可以推断最后一个单词可能是一种语言，但如果我们想要知道到底是哪种语言的话，我们需要句子最前面的一个单词“France”。这会使得相关信息以及需要预测词的位置的间隔变得很大。遗憾的是，随着间隔的逐渐增大，RNNs不能够去关联有用的信息。在理论上，RNNs能够解决长期依赖的问题。人们可以通过仔细选取参数来解决这类问题。但是实际上RNNs并不能去解决这个问题。 Hochreiter (1991) [German] and Bengio(1994)等人深入研究了该问题为何如此艰难。阿弥陀佛，LSTMs并没有上述问题。 LSTM网络长短期记忆网络——经常被叫做“LSTM”——是RNN的这一种特殊的形式，它能够解决长期依赖的问题。LSTM是由Hochreiter &amp; Schmidhuber (1997)提出，并由很多后来者完善以及推广。LSTM能够在很多问题上取得优秀的结果，现如今被广泛引用。LSTM被设计成防止长期依赖问题的发生。在实践中，LSTM的长期记忆是默认行为，而并非艰苦习得！所有的RNN都有链式重复的神经网络模块。在标准的RNN中，这些重复的模块仅仅有简单的结构，例如$tanh$层。当然，LSTM中也存在这样的链式结构，但是其中的重复模块就大为不同了。LSTM的重复的模块中包含4种不同的层，它们以一种特殊的结构交错着。看不懂，不用担心，细节即将展开。接下来，我们会一步步来讲解LSTM的网络结构。首先我们先明确几个会经常用到的表示方法。在以上的图示中，每条实线传输着整个向量，从一个节点的输出到其他节点的输入。粉红色的圆圈表示的是逐点操作，例如向量的加法，黄色的方形表示已经学习了的网络层。汇集的线表示串联，分叉的线表示复制操作，这些复制的内容流向不同的位置。 LSTM背后的核心技术LSTM的关键技术在于细胞（cell）状态，也就是图表中最上方水平穿行的直线。细胞状态可以理解成是一种传送带。它仅仅以少量的线性相交，贯穿整个链式结构上方。信息沿着这条传动带很容易保持不变。LSTM有一种能向细胞增加或者移除信息的能力，这种经过精心设计的结构称作门（gates）。所谓的门就是一种让信息选择性通过的方法。它是由一个$sigmoid$层和一个逐点乘法单元构成。如下图： $sigmoid$层的输出是$0$-$1$之间的数字，它描述了每个部分可以有多少量能够通过。$0$代表“啥都不能通过”，$1$代表“啥都能通过”！一个LSTM有三种这样的门结构，用来保证以及控制细胞状态。 逐步理解LSTMLSTM的第一步是来决定啥信息将要从细胞状态中丢弃。这个决定是由一个叫做“遗忘门”（“forget gate layer”）的$sigmoid$层来决定。它的输入是$h_{t-1}$和$x_t$，输出是一个介于$0$到$1$之间数值，给每个在状态$C_{t-1}$的数值。$1$表示“完全保留”，$0$表示“完全丢弃”。让我们回到之前的的语言模型的例子中，我们还是基于以前的词语来预测后续的单词。在这样一个问题中，细胞状态可能会包含当前主语的性别信息，所以正确的介词将会被使用。当我们看到一个新的主语时，我们要遗忘掉旧的的主语。 下一步就是决定啥新信息将要存储在细胞状态中。这包括两个方面。第一，一个叫做“输入门层”的$sigmoid$层来决定哪些值我们要更新；第二，一个$tanh$层创造了新的候选值$\tilde{C}_t$，这个值将会加入到新的状态中去。进一步，我们要把上述两个方面结合起来来更新细胞状态。在我们的语言模型中，我们想要在新的主语对应的细胞状态中加入性别信息，去代替我们遗忘掉的那个旧主语状态。 我们现在更新旧的细胞状态，从状态$C_{t-1}$到状态$C_{t}$。上述步骤已经详述了具体如何操作，我们现在就开始更新！我们将旧的细胞状态乘以$f_t$，目的是忘记我们要忘记的旧状态。然后我们加上$i_t*\tilde{C}_t$，这就是我们创造新的候选值，这个值根据我们想要更新每个状态值的程度进行伸缩变化（这就是$i_t$的意义）。在我们的语言模型中，这就是我们根据最开始确定的目标，丢弃旧主语性别以及增加新主语信息的地方。 最后一步我们要决定到底输出什么信息。这个输出信息会基于细胞状态，但将会是一个经过过滤后的结果。首先，我们用一个$sigmoid$层去决定细胞状态的哪一部分将会被输出。然后，我们将细胞状态通过$tanh$（将其值规范到$-1$到$1$之间）。最后我们将这个值与$sigmoid$输出相乘，这样我们仅仅输出我们想要输出的部分。还是对于之前提到的语言模型，因为它只看到了一个主语，它可能会输出一个与动词相关的信息。例如，可能输出这个主语是单数还是复数，所以我们会知道紧跟的动词是何种形式。至此，基本的LSTM介绍完毕。 LSTM的变体我们以上描述的均是最为普通的LSTM。但是并不是所有的LSTM都是以上那个样子。事实上，似乎每一篇涉及LSTMs的论文均对其做了细微的修改。其中的差别不大，以下列举几种LSTM的变体。 其中之一就是一种特别流行的LSTM变体，它由 Gers &amp; Schmidhuber (2000)提出，加入一种窥视孔连接（peephole connections）的结构。这使得细胞状态可以作为门层（译者：gete layers:$sigmoid$ layers &amp; $tanh$ layer）输入。以上的图示为每个门层加入了窥视孔连接，但是也有一些论文并不是所有的门层都加。 另外一种变体是加入了双遗忘门（coupled forget）以及输入门。我们同时考虑了何时遗忘以及应该加入何种新信息，而并非分别考虑。我们仅仅在我们需要就地输入信息的时候才会遗忘，同时我们仅仅在遗忘掉旧的信息的时候才会加入新的信息（译者：此时$f_t=0$，表示遗忘旧的细胞状态，同时加入新的输入$\tilde{C}_t$）。 另外一种改动较大的变体是门控循环单元（Gated Recurrent Unit）即GRU。这个算法由Cho,et al.(2014)提出，它把遗忘门和输入门结合起来构成一个“更新门”（update gate）。与此同时，它还将细胞状态和隐含状态合并起来，当然还有一些其他变化在此不一一赘述。最终的变体比标准的LSTM简单，这使得它很受欢迎。 以上均是最近比较劲爆的LSTM变体。当然也有很多其他形式的变体，如Yao,et al. (2015)提出的深度门RNN（Depth Gated RNNs）。还有一些变体用完全不同的方式来解决长期依赖问题，例如Koutnik,et al.(2014)提出的时钟频率驱动RNN（Clockwork RNNs ）。 列举了诸多LSTM变体，那么到底哪一种变体是最好的呢？其中的差异真的很重要吗？Greff, et al.(2015)做了一个非常棒了比较，发现这些变体几乎都是一样一样的。Jozefowicz,et al.(2015)测试了上万种RNN框架，发现了一些框架在特定任务上会比LSTM表现出色。（译者：没有一种算法一统江湖） 结论以上，我提到了人们利用RNNs得到了很多优秀的结果。在本质上说，几乎所有的RNNs都使用了LSTMs。LSTMs在诸多任务上表现优异。在介绍LSTMs的过程中写了很多公式，这让它看起来很吓人。幸运的是，我们在文中通过一步步地探索，让LSTM看起来平易近人。LSTMs是我们完成RNNs的重大成果。我们很自然地想：还有没有其他的重大成果？研究员们的共识是：当然有！下一个重大成果就是——注意力。这个观点是让RNNs的每一步都能够从更大的数据集中挑选信息。例如，如果你想利用RNNs去给一幅图像创造标题来描述它，这就可能会选择图像的一部分作为输入，然后根据这些输入来得到每个单词。事实上，Xu,et al.(2015)就是这么做的——这可能你探究注意力这个领域的起点。还有诸多使用注意力取得的令人激动的研究结果，看起来还有更多需要探索。注意力并非RNN唯一令人激动的研究方向。例如，Kalchbrenner, et al. (2015)提出的网格LSTM（Grid LSTMs）看似非常有前景。Gregor, et al.(2015)，Chung, et al.(2015)和 Bayer &amp; Osendorfer (2015)等人的研究工作是在生成模型中使用RNNs，这些工作都看起来非常有趣。过去几年是RNNs异常火爆的时期，未来也会有更多更加有意义的成果出现。 致谢我非常感谢那些帮助我去理解LSTMs的大佬们，同时感谢对可视化进行评论并在这篇博文提供反馈的网友们。非常感激谷歌同事们的反馈，尤其感谢Oriol Vinyals，Greg Corrado，Jon Shlens，Luke Vilnis以及Ilya Sutskever。我也由衷感谢很多同事朋友的帮助，包括Dario Amodei和Jacob Steinhardt。值得特别感谢还有Kyunghyun Cho，这哥们对图表的绘制给了我极大的帮助。在写这篇博文之前，我尝试在我讲授的神经网络课程中利用两系列研讨会的时间来解释LSTMs。感谢每一位参与者，感觉大家的反馈。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[降维之PCA主成分分析原理]]></title>
      <url>%2F2017%2F10%2F02%2F%E9%99%8D%E7%BB%B4%E4%B9%8BPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%8E%9F%E7%90%86%2F</url>
      <content type="text"><![CDATA[背景在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。 目的PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。能够对高维数据降维的算法包括： LASSO 主成分分析法 聚类分析 小波分析法 线性判别法 拉普拉斯特征映射 降维有什么作用降维有什么作用呢？ 数据在低维下更容易处理、更容易使用 相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示 去除数据噪声 降低算法开销 常见的降维算法有主成分分析（principal component analysis,PCA）、因子分析（Factor Analysis）和独立成分分析（Independent Component Analysis，ICA）。 优化目标将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。注意：PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 PCA原理 最大化样本点在基上的投影，使得数据点尽量的分离。令第一主成分的方向是u_1，我们的目标就是将样本点在该方向上的投影最大化，即： \max \frac{1}{n}\sum_{i=1}^n^2 \frac{1}{n}\sum_{i=1}^n \rightarrow \frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^2=\frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^T(x_1^Tu_1) =\frac{1}{n}\sum_{i=1}^n(u_1^Tx_1x_1^Tu_1)=\frac{1}{n}u_1^T\left(\sum_{i=1}^nx_1x_1^T\right)u_1=\frac{1}{n}u_1^T\left(XX^T\right)u_1其中的X=[x_1,x_2,...,x_n]^T,x_i\in R^{m}。那么优化函数就变成了： \max u_1^T\left(XX^T\right)u_1以上式子是个二次型，可以证明XX^T是半正定矩阵，所以上式必然有最大值。 \max u_1^T\left(XX^T\right)u_1=\max ||X^Tu_1||_2^2优化函数 max||Wx||_2 s.t. W^TW=I解释：最大化方差同时最小化协方差（PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”）。最大化方差意味着，使得每个样本点在每个维度上与均值有很大差异，就是说非常有个性，有个性才能分辨出来；同时协方差越小的话表明样本之间的互相影响就非常小，如果协方差是0的话，表示两个字段完全独立。 寻找协方差矩阵的特征向量和特征值就等价于拟合一条能保留最大方差的直线或主成分。因为特征向量追踪到了主成分的方向，而最大方差和协方差的轴线表明了数据最容易改变的方向。根据上述推导，我们发现达到优化目标就等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将特征值按大小从上到下排列。协方差矩阵作为实对称矩阵，其主要性质之一就是可以正交对角化，因此就一定可以分解为特征向量和特征值。 具体实施步骤总结一下PCA的算法步骤，设有m条n维(字段数)数据，我们采用以下步骤对数据降维。 将原始数据按列组成n行m列矩阵X. (行数代表字段数目，一个字段就是取每个样本的该维度的数值；列数代表样本数目) 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 求出协方差矩阵C=\frac{1}{m}XX^T 求出协方差矩阵的特征值及对应的特征向量 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P Y=PX即为降维到k维后的数据 去均值化的目的下面两幅图是数据做中心化（centering）前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是(0,0). 在做PCA的时候，我们需要找出矩阵的特征向量，也就是主成分（PC）。比如说找到的第一个特征向量是a = [1, 2]，a在坐标平面上就是从原点出发到点（1，2）的一个向量。如果没有对数据做中心化，那算出来的第一主成分的方向可能就不是一个可以“描述”（或者说“概括”）数据的方向了。还是看图比较清楚。 黑色线就是第一主成分的方向。只有中心化数据之后，计算得到的方向才能比较好的“概括”原来的数据。 限制PCA虽可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关。 参考 PCA的数学原理 K-L变换和PCA的区别 从PCA和SVD的关系拾遗 数据什么时候需要做中心化和标准化处理 主成分分析（PCA）原理详解 附上最近比较火的一首歌Time]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SIFT和SURF特性提取总结]]></title>
      <url>%2F2017%2F10%2F01%2FSIFT%E5%92%8CSURF%E7%89%B9%E6%80%A7%E6%8F%90%E5%8F%96%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[SIFT（Scale-invariant feature transform）是一种检测局部特征的算法，该算法通过求一幅图中的特征点（interest points,or corner points）及其有关scale 和 orientation 的描述子得到特征并进行图像特征点匹配 什么是SIFT先看看上图利用sift进行匹配的结果： 这个结果应该可以很好的解释sift的尺度、旋转以及光照不变性。接下来就介绍一下这个神奇的算法的奥义。我把代码放在了Github，感兴趣的同学自己下载下来试试。 算法描述SIFT特征具有尺度不变性，旋转不变性，光照不变性。 实现流程构建尺度空间尺度空间的目的是模拟图像的多尺度特性。高斯卷积核是实现尺度变换的唯一线性核，于是 一副二维图像的尺度空间定义为： L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)其中的G(x,y,\sigma)是尺度可以发生变化的高斯函数G(x,y,\sigma)=\frac{1}{2\pi{\sigma}^2}e^{-\frac{x^2+y^2}{2{\sigma}^2}}。(x,y)表示空间坐标，\sigma是尺度系数，描述了图像的模糊程度。为了能够更为有效的提取出特征点，提出了DOG（高斯差分尺度空间）的概念。通过不同尺度下的高斯差分核与图像卷积形成： D(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) =L(x,y,k\sigma)-L(x,y,\sigma) 图像金字塔的建立：为了实现尺度不变特性，对于每一幅图像I(x,y)，分成子八度（octave），第一个子八度的scale为原图大小，后面每个octave为上一个octave降采样的结果，即原图size的1/4（长宽分别减半），构成下一个子八度（高一层金字塔）。此时要强烈注意size和尺度空间的概念。size是图像大小，而尺度空间表示不同\sigma的图像的集合。那么尺度空间的集合是： 2^{i-1}(\sigma, k*\sigma,k^2*\sigma,k^3*\sigma,...,k^{n-1}*\sigma)其中的 k=2^{1/S}，S表示尺度金字塔每个octave的层数，n表示尺度金字塔的总层数，i表示的是在某个octave的第i层，i\in[1,2,3,...n]。 由图片size决定建几个塔，每塔几层图像(S一般为3-5层)。0塔的第0层是原始图像(或你double后的图像)，往上每一层是对其下一层进行Laplacian变换（高斯卷积，其中σ值渐大，例如可以是σ, k*σ, k*k*σ…），直观上看来越往上图片越模糊。塔间的图片是降采样关系，例如1塔的第0层可以由0塔的第3层down sample得到，然后进行与0塔类似的高斯卷积操作。 在DoG空间找到关键点为了寻找尺度空间的极值点，每一个采样点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点,如图所示。使用Laplacian of Gaussian能够很好地找到找到图像中的兴趣点，但是需要大量的计算量，所以使用Difference of Gaussian图像的极大极小值近似寻找特征点.DOG算子计算简单，是尺度归一化的LoG算子的近似。 去除不好的点 这一步本质上要去掉DoG局部曲率非常不对称的像素。通过拟和三维二次函数以精确确定关键点的位置和尺度（达到亚像素精度），同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力，在这里使用近似Harris Corner检测器。 给特征点赋值一个128维方向参数并描述前面的几个步骤确定了特征点到底在哪里，此步骤是为了描述特征点。(x,y)处梯度的模值和方向公式为： m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2} \theta(x,y)=tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right) 利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性。 其中L所用的尺度为每个关键点各自所在的尺度。至此，图像的关键点已经检测完毕，每个关键点有三个信息：位置，所处尺度、方向，由此可以确定一个SIFT特征区域。 在实际计算时，我们在以关键点为中心的邻域窗口内采样，并用直方图统计邻域像素的梯度方向。梯度直方图的范围是0～360度，其中每45度一个柱，总共8个柱, 或者每10度一个柱，总共36个柱。Lowe论文中还提到要使用高斯函数对直方图进行平滑，减少突变的影响。直方图的峰值则代表了该关键点处邻域梯度的主方向，即作为该关键点的方向。直方图中的峰值就是主方向，其他的达到最大值80%的方向可作为辅助方向。 计算keypoint周围的16*16的window中每一个像素的梯度，而且使用高斯下降函数降低远离中心的权重。图左部分的中央为当前关键点的位置，每个小格代表关键点邻域所在尺度空间的一个像素，利用公式求得每个像素的梯度幅值与梯度方向，箭头方向代表该像素的梯度方向，箭头长度代表梯度模值，然后用高斯窗口对其进行加权运算。 该图是8*8的区域计算得到2*2描述子向量的过程。但是在实际中使用的是在16*16的区域计算得到4*4的特征描述子，如下图： 这样就可以对每个feature形成一个4*4*8=128维的描述子，每一维都可以表示4*4个格子中一个的scale/orientation。将这个向量归一化之后，就进一步去除了光照的影响。 sift的缺点SIFT在图像的不变特征提取方面拥有无与伦比的优势，但并不完美，仍然存在： 实时性不高。 有时特征点较少。 对边缘光滑的目标无法准确提取特征点。 PS: 论文见这里：Distinctive Image Features from Scale-Invariant Keypoints这里是David Lowe大神做的一个Demo Software: SIFT Keypoint Detector. SURF 简介参考了好友整理的一篇文章特征与匹配 整体的思路就是将计算DOG的一整套东西来检测关键点的理论替换成了利用hessian矩阵来检测关键点，因为当Hessian矩阵的判别式取得局部极大值时，判定当前点是比周围邻域内其他点更亮或更暗的点，由此来定位关键点的位置。上述过程要进行Hessian判别式的计算，可以采用box filter的方式进行加速。 构建尺度金字塔的方式不同，具体见下图： Sift特征点方向分配是采用在特征点邻域内统计其梯度直方图，取直方图bin值最大的以及超过最大bin值80%的那些方向作为特征点的主方向。而在Surf中，采用的是统计特征点圆形邻域内的harr小波特征。即在特征点的圆形邻域内，统计60度扇形内所有点的水平、垂直harr小波特征总和，然后扇形以0.2弧度大小的间隔进行旋转并再次统计该区域内harr小波特征值之后，最后将值最大的那个扇形的方向作为该特征点的主方向。该过程示意图如下： 生成特征点描述子: 在Sift中，是取特征点周围4*4个区域块，统计每小块内8个梯度方向，用着4*4*8=128维向量作为Sift特征的描述子。surf算法中，也是在特征点周围取一个4*4的矩形区域块，但是所取得矩形区域方向是沿着特征点的主方向。每个子区域统计25个像素的水平方向和垂直方向的haar小波特征，这里的水平和垂直方向都是相对主方向而言的。该haar小波特征为水平方向值之后、垂直方向值之后、水平方向绝对值之后以及垂直方向绝对值之和4个方向。把这4个值作为每个子块区域的特征向量，所以一共有4*4*4=64维向量作为Surf特征的描述子，比Sift特征的描述子减少了2倍。 参考 SIFT特征提取分析 特征匹配-SURF原理与源码解析（一） 特征与匹配]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[统计学习方法总结]]></title>
      <url>%2F2017%2F09%2F23%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[本文主要研究监督学习，所谓的监督学习就是在给定的，有限的，用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个集合，即假设空间；我们根据一定的评价准则，从假设空间中选取一个最优的模型，使它对已知的训练数据以及未知的测试数据在给定评价准则下有最优的预测，最优模型的选取由算法实现。所以统计学习方法有三个要素：模型、策略、算法。 统计学习 监督学习 半监督学习 无监督学习 强化学习 输入空间、特征空间与输出空间 输入变量&amp;输出变量均连续-&gt; 回归问题 输出空间为有限个离散变量的预测问题-&gt; 分类问题 输入与输出均为变量序列的预测问题-&gt; 标注问题 风险函数 期望风险：模型关于联合分布的期望损失 经验风险：模型关于训练样本的平均损失按照大数定律，当样本数据量区域无穷时，经验风险趋近于期望风险；但是当样本容量很小时，经验风险的效果就不会太好，此时容易出现过拟合现象。此时，结构风险就被提出。结构风险是在经验风险的基础上添加上表示模型复杂度的正则化项/罚项。极大似然估计是经验风险最小化的一个特例。最大后验概率估计是结构风险最小化的一个特例； 模型监督学习里要学习的模型就是决策函数或者条件概率分布。 此时不得不提到生成方法以及判别方法。 生成方法，由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布模型，即生成模型： P(Y|X)=\frac{P(X,Y)}{P(X)} 判别方法是由数据直接学习决策函数或者条件概率分布作为预测模型，即判别模型。 生成模型与判别模型 生成模型常见的主要有： 高斯混合模型 朴素贝叶斯 混合高斯模型GMM 隐马尔可夫模型HMM 马尔可夫的随机场 KNN 常见的判别模型有： 支持向量机 传统的神经网络 线性判别分析 线性回归 条件随机场 最大熵模型 逻辑斯特回归 策略指定策略的目的就是为了挑选出假设空间中到底哪个模型才是我们真正需要的。此时会用到损失函数以及风险函数的概念。 0-1 损失函数 L(Y,f(X))=\left\{\begin{aligned} 1, && Y \neq f(X)\\ 0, &&Y = f(X) \end{aligned}\right. 平法损失函数 L(Y,f(X))=(Y-f(X)^2 绝对值损失函数 L(Y,f(X))=|Y-f(X)| 对数损失函数 L(Y,P(Y|X))=-logP(Y|X) 损失函数越小的话代表模型越好。$(X,Y)$是随机变量符合联合分布概率$P(X,Y)$，所以损失函数的期望被定义为： R_{ref}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)dxdy以上是模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或者期望损失（expected loss）。但是呢，期望损失不易求解，我们一般用模型关于训练数据集的平均损失来逼近期望损失，即： R_{emp}(f)=E_p[L(Y,f(X))]=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))经验风险最小化的策略认为，经验风险最小的模型就是最优的模型，于是按照这种定义，我们有： f^*={argmin}_{f \in \mathcal{F} } R_{emp}其中的$\mathcal{F}$是假设空间。最大似然估计就是经验风险最小化的一个例子：当模型为条件概率，损失函数是对数损失时，经验风险最小化就等价于极大似然估计。根据大数定理可知，当样本容量N趋近于无限时，经验风险趋近于期望风险。但是如果样本数量是有限时，此时会出现过拟合现象，那么这时候需要结构风险的帮助。结构风险是为了防止过拟合而提出的策略，结构风险最小化等价于正则化（regularization）。其定义是 R_{srm}(f)=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)其中的$J(f)$是衡量模型复杂度的项，也叫罚项。当模型越复杂时，$J(f)$越大；模型越简单时，$J(f)$越小。最大后验概率估计（MAP）就是结构风险最小化的一个例子：当模型时条件概率，损失函数是对数损失函数，模型复杂度由先验概率表示时，结构风险最小化就等价于MAP。 监督学习的模型可以分为概率模型与非概率模型，由条件概率分布$P(Y|X)$或者决策函数$Y=f(X)$表示。 算法算法是指学习模型的具体计算方法： find global optimization solution efficiently. 模型选择模型选择的主要方式有：正则化与交叉验证。 在经验风险最小化时已经了解到正则化的由来，正则化就是针对过拟合现象提出的解决策略。 过拟合是指学习模型时选择的模型包含的参数过多，以致于这一模型对已知数据的预测很好，而对未知数据的预测能力变得很差。 以下介绍两种正则化的范数：$L_1$ and $L_2$ 正则化L2范数正则 C=C_0+ \frac{\alpha}{2n}\sum_ww^2对$w$以及$b$求导如下： \frac{\partial C}{\partial w}= \frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w \frac{\partial C}{\partial b}= \frac{\partial C_0}{\partial b}由梯度下降法可知： w \rightarrow w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n} \right)w-\eta\frac{\partial C_0}{\partial w}系数$\left(1-\frac{\eta\lambda}{n} \right)$是小于1的，相比于原来的系数等于一，此时的效果相当于就是权值衰减（weight decay）。 注意到过拟合的时候，我们的假设函数要顾及到每一个数据点，模型就会尝试对所有数据点进行拟合，包括一些异常点；此时形成的拟合函数的波动性会非常大，可以看到此时的拟合参数会异常大。通过L2正则化处理可以使这些参数变小，解释如下： 更小的权重，表示网络的复杂组更低，对数据拟合的刚刚好（奥卡姆剃须刀原理） L1范数正则L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0；L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小 参考链接lasso and ridge regularization 交叉验证就是将整个数据集切分成三个部分，分别是训练集、验证集和测试集。训练集用来训练模型，验证集用于模型 的选择，而测试集用于对学习方法的评估。 但是一般情况下，训练数据是不足的，那么此时可以重复的利用数据，进行反复训练以得到最优模型。 常见的方法有： S折交叉验证（S-fold cross validation） 留一交叉验证（S=N，当S为数据集的容量时，S折交叉验证就变成了留一交叉验证） 极大似然估计最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知” 举个别人博客中的例子，假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？很多人马上就有答案了：70%。而其后的理论支撑是什么呢？ 我们假设罐中白球的比例是$p$，那么黑球的比例就是$1-p$。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜 色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率$P(Data|M)$，这里Data是所有的数据，M是所给出的模型，表示每次抽出来的球是白色的概率为$p$。如果第一抽样的结果记为x_1，第二抽样的结果记为x_2… 那么Data = (x_1,x_2,...,x_{100})。这样， P(Data|M)= P(x_1,x_2,...,x_{100}|M)= P(x_1|M)P(x_2|M)...P(x_{100}|M)= p^{70}(1-p)^{30}那么p在取什么值的时候，$P(Data|M)$的值最大呢？将$p^{70}(1-p)^{30}$对$p$求导，并其等于零。 70p^{69}(1-p)^{30}-p^{70}*30(1-p)^{29}=0解方程可以得到p=0.7。 最大熵原理最大熵原理使概率学习中的一个准则。学习概率模型时，在所有的可能概率模型（分布）中，熵最大的模型是最好的模型。最大熵原理也可以理解成在满足约束条件的模型中选择熵最大的模型！ H(p)=-\sum_x{log(P(x))P(x)}其中$X$服从的概率分布为$P(X)$，$X$服从均匀分布时，熵最大。当没有其他已知的约束时，$\Sigma{P(x)}=1$，此时按照最大熵原理，当$P(x_1)=P(x_2)=…=P(x_n)$时，熵最大；此时等概论，等概论意味着对事实的无知，因为没有更多可能的信息，所以此时的判断也是合情合理的。 线性分类器线性分类器有三大类：感知器准则函数、$SVM$、$Fisher$准则，而贝叶斯分类器不是线性分类器。 感知器准则函数：代价函数$J=-(W*X+w_0)$，分类的准则是最小化代价函数。感知器是神经网络（$NN$）的基础，网上有很多介绍。 $SVM$：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题） $Fisher$准则：更广泛的称呼是线性判别分析（$LDA$），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 评价指标 召回率就是有多少正确的被你找回来了；准确率就是你找到的有多少是正确的；(一般情况下召回率和准确率呈负相关，所以可以用F值衡量整体效果) TP(True Positive)是你判断为正确的，实际就是正确的； FP(False Positive)是你判断是错误的，实际也是错误的； TN(True Negative)是你判断为正确的，但实际是错误的； FN(False Negative)是你判断是错误的，但实际是正确的； 朴素贝叶斯 Naive Bayes基本方法朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。对于由$P(X,Y)$独立产生的训练集$T={(x_1,y_1),(x_2,y_2),…, (x_N,y_N),}$而言通过朴素贝叶斯的方法学习这个联合概率分布。大致分两步： 计算先验分布： P(Y=c_k),k=1,2...,K 条件概率分布： P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2...,K其中的$x \in R^{n}$，$n$表示这个样本的维度。 但是在计算条件概率时因为朴素贝叶斯做了条件独立性假设，那么该条件概率分布可以写成： P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k) \\ =\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)在实际分类时，对于给定的输入x，通过学习到的模型估计后验概率$P(Y=c_k|X=x)$将后验概率最大的类作为x的类的输出。 P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}\ =\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}因为分母就是$P(X=x)$的概率，这是不变的。因此我们仅需要知道分子哪个大就可以，也就是： y={argmax}_{c_k} P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)上式的意思是求解到底是哪些类别c_k能够让最大后验概率最大化。 参数估计极大似然估计 计算先验概率 P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N} 计算条件概率 P(X=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}其中 j=1,2...n;l=1,2...S_j;k=1,2...K 对于给定的实例x，计算 P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k) 确定实例的类别]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu上使用Git以及GitHub]]></title>
      <url>%2F2017%2F05%2F30%2FUbuntu%E4%B8%8A%E4%BD%BF%E7%94%A8Git%E4%BB%A5%E5%8F%8AGitHub%2F</url>
      <content type="text"><![CDATA[安装Git在Ubuntu上安装Git的命令为: 1sudo apt-get install git 配置GitHub安装git结束之后就是配置github用户资料，如下：将其中的 “user_name”替换成自己 GitHub的用户名并且将”email_id” 换成你创建GitHub账号所用的邮箱.123git config --global user.name &quot;user_name&quot;git config --global user.email &quot;email_id&quot; 建立本地仓库（repository）在自己的电脑上建立本地仓库，这个仓库将会在后续推送到GitHub上，语句如下：1git init Mytest 如果初始化成功，你将会得到以下提示： 1Initialized empty Git repository in /home/user_name/Mytest/.git/ 其中的user_name为本地计算机用户名，因人而异。Mytest是”init”创建的文件夹，然后进入该文件所在目录：1cd Mytest 创建README来描述这个仓库该步骤可有可无，但是作为一个优秀的工程师还是写点东西比较好。 1gedit README 然后输入：1This is a git repo 将仓库文件加入index（缓存）这一步尤其重要，我们将需要上载到GitHub的文件们添加到index。这些文件可以是文本文档，m/c/c++/PDF/jpg…几乎任何类型文件，一般而言我们可以把需要上载的文件拷贝到这个文件夹内，然后再用一个语句来把需要上传到文件添加到index，如下：1234git add file1.txtgit add file2.cgit and file3.m... 提交到本地仓库当我们已经把文件添加／修改到index后，就可以进行提交了；利用如下语句： 1git commit -m &quot;some_message&quot; 其中some_message可以是任何字符，例如：”my first commit” 或者”edit in readme”等。上面代码的-m参数，就是用来指定这个mesage 的。 注意：git是分为三部分，一部分是文件，另外一个是缓存区，最后一个是本地库。当你修改了自己的文件后，你会git add xx将修改保存到缓存区，然后再用commit推送修改到本地库中。git push 将本地仓库修改推送到服务器上的仓库中commit是将本地修改保存到本地仓库中。 在GitHub创建仓库这个仓库的名字要和本地的一致，该部分不做展开。然后连接到远程仓库 1git remote add origin https://github.com/user_name/Mytest.git 其中user_name就是自己的GitHub的用户名。 推送到远程仓库最后的一步就是提交到远程仓库，用以下命令：1git push origin master 原文地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习修炼手册]]></title>
      <url>%2F2017%2F05%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BF%AE%E7%82%BC%E6%89%8B%E5%86%8C%2F</url>
      <content type="text"><![CDATA[对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。 机器学习算法包括，监督学习(回归、分类)以及非监督学习(聚类)。 梯度下降\bbox[5px,border:2px solid red] { \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta}J(\theta) }其中$\alpha$为学习率一般为很小的数字(0.001-0.1)，$\theta$为我们需要求解的参数，$J(\theta)$为能量函数或者为损失函数，通过上述公式可知，梯度下降是沿着损失函数梯度的反方向寻找迭代寻找最优值的过程。梯度下降容易陷入局部最极小点，所以我们要采取一定的措施来阻止这种现象的发生。 过拟合（Overfitting）如果训练样本的特征过多，我们学习的假设可能在训练集上表现地很好，但是在验证集上表现地就不尽人意 避免过拟合 减少特征的大小 正则化 在保证所有特征都保留的情况下，限制$\theta$的大小，即Small values for parameters $ \theta_0,\theta_1,\theta_2…\theta_n$ 当特征量很多时，该方式仍然表现的很好 交叉验证(Cross Validation) 正则化线性回归对于线性回归而言，其损失函数形式如下： J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2引入正则化之后的损失函数的形式为： J(\theta)=\frac{1}{2m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2\right)GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)} \theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)}梯度下降法的学习率$\alpha$需要提前指定，并且还要制定收敛标准。 Normal Equation \theta=\left(x^Tx+\lambda\begin{bmatrix} {0}&{0}&{\cdots}&{0}\\ {0}&{1}&{\cdots}&{0}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {0}&{0}&{\cdots}&{1}\\ \end{bmatrix}_{(n+1)(n+1)}\right)^{-1}x^Ty上式是对线性回归正则化后的矩阵解。可以证明的是当$\lambda&gt;0$时，求逆符号内部的式子总是可逆的。 逻辑回归在没有加入正则化之前，逻辑回归的损失函数的形式是这样的： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)加入正则项之后的形式为： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}\theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)} SVM支持向量机支持向量机又被称作最大间距（Large Margin）分类器，损失函数的形式是： J(\theta)=C\sum_{i=1}^{m}\left(y^{(i)}cost_1\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})cost_0\left(h_{\theta}(x^{(i)})\right)\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2其中：$h_{\theta}(x^{(i)})=\theta^Tx^{i}$，$cost_1$以及$cost_0$的形式如下图所示： \begin{cases} \text{we want } \theta^Tx\ge1, & \text{if $y$ =1} \\[2ex] \text{we want } \theta^Tx\le-1, & \text{if $y$ =0} \end{cases}在考虑到soft margin时的损失函数是hinge损失，SVM就等价于Hinge损失函数+L2正则。此时损失函数为0时候就对应着非支持向量样本的作用，“从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率”。以下是七月在线大神July写的一篇关于SVM的介绍，个人觉得不错。分享下：支持向量机通俗导论（理解SVM的三层境界）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux指令学习笔记]]></title>
      <url>%2F2017%2F05%2F06%2FLinux%E6%8C%87%E4%BB%A4%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[今天看了基本的Linux指令以及makefile的写法，大体总结了一下。 常用指令及意义 root 表示根目录 cd path 切换到path目录， cd / 切换到根目录 cat file.txt 查看file.txt中的内容 pwd查看当前所在目录 rmdir删除目录 rm 删除文件 ls 列出文件名字， ls -l 列出文件列表 cp 复制文件 cp file1.txt file2.txt (复制file1并重命名为file2) head file.txt -n 7 查看file.txt的头7行 tail file.txt -n 7 查看file.txt的末尾7行 wc file.txt 统计file.txt文件中字符数，返回3个数字：row_size, word_number, character_number; -l 统计行数。即 wc -l file.txt-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。 mv 命令有2中功能 移动文件夹：mv file.txt file 修改文件名: mv file1.txt file2.txt chmod 修改用户权限，有3种用户，分别是： 解释 u g o 用户 user 作者 group小组 other其他 读写运行 r w x r w x r w x 二进制 1 0 0 1 0 0 1 0 0 十进制 4 4 4 假如取消作者的写的权限则：chmod u-w file.txt，其中减号表示“去除”假如添加作者的写的权限则：chmod u+w file.txt，其中减号表示“添加”假如除了作者以外的人都没有读写权限：chmod go-r file.txt同样可以用二进制设置权限 chmod 444 file.txt，表示：u,g,o都只有读的权限。 脚本语言就是将Linux命令集中在一起，组成一个文件，例如有一个test.sh脚本文件123lsdatecal 后缀名 .sh 运行 sh test.sh 变量赋值不用加空格, b=1, a=$b 输出 echo 字符串写不写双引号一样效果，但是还是推荐写双引号 大于号 -gt 小于号 -lt 等于号 -eq 大于等于 -ge 小于等于 -le 不等于 -ne 判断语句123456if [ expr ]then...else...fi 循环语句1234for x in 1 2 3 4 5 6do echo $xdone 数组例如：arr=(1 2 3 5 6 3 4)注意在运行时候不能继续用sh (1979脚本)；应该改用bash (后期脚本，有数组的时候就用bash)。 12345678910arr=(1 2 5 9 8 6 5 4 3 2)max=$&#123;arr[0]&#125;for i = $&#123;arr[@]&#125;do if [ $i -gt $max ] then max=$i fidoneecho max= $max 全局变量 $USER $HOME 可以用 ~ 代替 环境变量 $PATH,将一个路径加入这个全局变量:PATH=$PATH:/home/vincent/tutorial (注意所有的路径都是用冒号间隔开的，) 常用指令压缩 把几个文件打包成file.zip, zip file.zip * (星号的意思是打包所有的文档) 把全部的子文件夹都递归打包 zip file.zip -r files/* 利用tar命令： tar -zcvf file.tar.gz files/ 解压 利用tar命令： tar -zxvf file.tar.gz 下载命令 wget 下载后重命名 wget web_address -O file.tar.gz 注意用-O makefile的写法当编译代码的时候，如果有很多子文件，gcc 后面的语句非常长，我们可以选择使用makefile来对其进行处理以加速编译速度并加强可读性。基本的语句是如下所示的格式：12Target: dependencies(tab键) command 其中Target表示目标，例如最后想把所有的.c .o 文件们打包成main,那么Target就是main dependencies表示依赖项们，即所有的.c .h .o command为命令即如：gcc test.c -o test 例如我们想把tool.c 和main.c 打包成main.o的目标文件, 则makefile的写法为：12main: main.c tool.o gcc main.c tool.o -o main 但是我们发现并没有tool.o文件所以，还要把tool.o怎么来的说明一下：12tool.o: tool.c gcc -c tool.c 注意：gcc -c表示直接把tool.c编译成目标文件tool.o最后呢，如果代码开源的话一般不需要保留.o文件以及main，所以最后还需要把所有的.o以及main文件删除，我们需要在makefile文件的最后添加如下代码：12clean: rm *.o main 最后这个makefile可以写成：123456main: main.c tool.o gcc main.c tool.o -o maintool.o: tool.c gcc -c tool.cclean: rm *.o main 如果编译器不是gcc，而是其他的编译器，这时候我们有必要做一下代换来提高代码的可移植性。令：CC=gcc， 在引用的时候 $(CC)1234567CC=gccmain: main.c tool.o $(CC) main.c tool.o -o maintool.o: tool.c $(CC) -c tool.cclean: rm *.o main 例如有test1.c，test2.c，它们分别实现了查找最大值和最小值的功能；然后我们把这两个函数的声明分别放在Max.h和Min.h里面，最后我们在主函数main.c里面包含这两个头文件，然后调用这个两个求最大最小值的函数。 123456789CC=gccmain: main.c Max.o Min.o $(CC) main.c Max.o Min.o -o mainMax.o: test1.c $(CC) test1.c -o MaxMin.o: test2.c $(CC) test2.c -o Minclean: rm *.o main 如果还有第三方的库文件，我们如何链接呢？12345678910CC=gccCFLAGS=-lm -Wall -gmain: main.c Max.o Min.o $(CC) $(CFLAGS) main.c Max.o Min.o -o mainMax.o: test1.c $(CC) $(CFLAGS) test1.c -o MaxMin.o: test2.c $(CC) $(CFLAGS) test2.c -o Minclean: rm *.o main 未完待续，需要学习的知识太多了，深深地感觉到了自己的无知:(]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[初试HCI光场数据集]]></title>
      <url>%2F2017%2F04%2F30%2F%E5%88%9D%E8%AF%95HCI%E5%85%89%E5%9C%BA%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
      <content type="text"><![CDATA[好的数据集是做出漂亮实验的必要条件 声明：一切理解都是本人观点，如有错误，还望指出。如需转载请与本人联系，谢谢合作！邮箱：qin123yw@163.com Wanner光场数据集目前光场数据集有如下几种主流的数据集： 斯坦福大学光场数据集 Wanner(HCI)数据集(Old 4D Light Field Benchmark) 4D Light Field Dataset(Konstanz大学与Heidelberg大学的HCI合作,New 4D Light Field Benchmark) 下面对Wanner数据集进行讨论。学习光场的同学应该很熟悉Wanner提供的数据集共有10个场景，分别是： Buddha Buddha2 Couple Cube Mona Medieval Papillon StillLife Horses rx_watch 其中，1-8为仿真场景，9-10是由Raytrix拍摄的场景。他们的文件后缀为 .h5, 格式是HDF5，这是一种文件组织格式，可以很好的将数据组织在一起，具体不做展开。MATLAB 提供了一系列相应的读取该文件的函数，如：h5disp，hdf5info(新版本用h5info)，hdf5read等函数，如利用h5disp就可以得到HDF5文件的内容信息，如下图： 以下给出解码HDF5文件得到子孔径图像以及重排图像的代码： 123456789101112131415161718192021222324252627input_file = 'Buddha2.h5'; % file nameinput_folder = ''; % your datasets folder[pathstr,name,ext] = fileparts([input_folder '/' input_file]);file_path=[pathstr,name,ext];hinfo_data = hdf5info(file_path);if strcmp(file_path,'Cube') || strcmp(file_path,'Couple') data = hdf5read(hinfo_data.GroupHierarchy.Datasets(3));else data = hdf5read(hinfo_data.GroupHierarchy.Datasets(2));enddata = permute(data, [3 2 1 5 4]);data = im2double(data(:, :, :, :, end:-1:1));% parameters from inputUV_diameter = size(data, 4); % angular resolutionUV_radius = floor(UV_diameter/2); % half angular resolutionh = size(data, 1); % spatial image heightw = size(data, 2); % spatial image widthy_size=h;x_size=w;UV_size=UV_diameter^2;LF_y_size = h * UV_diameter; % total image heightLF_x_size = w * UV_diameter; % total image widthLF_Remap = reshape(permute(data, ... [4 1 5 2 3]), [LF_y_size LF_x_size 3]); % the remap imageIM_Pinhole = data(:,:,:,UV_radius+1,UV_radius+1); % the pinhole image 经过以上步骤可以得到相应的中心视角图像以及Remap（重排）之后的图像，从而进一步方便接下来的工作，例如基于该数据集的深度图像估计算法估计。 HCI 4D光场数据集(4D Light Field Benchmark)The 4D Light Field Benchmark was jointly created by the University of Konstanz and the HCI at Heidelberg University. 上周整理上一篇博客的时候，想再次查看HCI数据集是否更新，结果惊喜地看到它竟然更新了！激动之余，就连夜把数据以及代码下载了下来，看看这个数据集的庐山真面目。 数据集概况这个数据集共有4大类： Stratified（4） training（4） test（4） additional（16） 总结而言这个4D光场数据集提供了如下信息： 9x9x512x512x3 light fields as individual PNGs（角度分辨率：9×9，空间分辨率：512×512） Config files with camera settings and disparity ranges（相机配置文件以及视差范围） Per center view (except for the 4 test scenes):（除了测试类外每类的中心视角图像） 512×512 and 5120×5120 depth and disparity maps as PFMs（深度图像以及视差图：512×512低分辨率，以及5120×5120高分辨率） 512×512 and 5120×5120 evaluation masks as PNGs（png格式的评价掩膜：512×512低分辨率，以及5120×5120高分辨率） 16组additional的每个视角的Ground Truth Depth图像（pfm格式） 数据集下载开始下载吧！在该页面的get the data处填写自己的邮箱，然后点击request download links。接下来你的邮箱里就会出现这个数据集的下载链接，链接有点多，你可以选择性的下载或者全部下载。方便起见，我把邮件中提供的链接贴在了下面： Benchmark package with the 12 benchmark scenes Full package with all 28 scenes(这是全部的场景，共28类；注意不包含深度图像) Packages per category: stratified test training additional Stratified scenes: backgammon dots pyramids stripes Test scenes: bedroom bicycle herbs origami Training scenes: boxes cotton dino sideboard Additional scenes: antinous boardgames dishes greek kitchen medieval2 museum pens pillows platonic rosemary table tomb tower town vinyl Depth and disparity maps for all views of the additional scenes 数据集初体验测试代码下载在其官方给出的代码页面下载测试程序，下载完毕后将convert*.m以及lib文件夹其放置在与上述数据集同级目录。例如：TEST目录下同时包括：convert.m 以及 lib/， 同样也包含 additional/, stratified/, test/, 以及 training/。 生成LF.mat convertAll. 对于每一个场景都声称一个LF.mat文件 如果我们仅仅下载了几个场景我们可以利用如下函数得到相应的LF.mat convertBlenderTo5D(‘FOLDER’) 这个LF.mat中包含该场景的光场信息诸如： 光场数据 (LF.LF) 真实值 (LF.depth/disp_high/lowres) 评价掩膜（mask） 中心视角图像 注意：生成LF.mat的过程用到的参数通过加载相应文件夹下parameters.cfg得到，并将其存储在了LF.parameters中；H变换矩阵存储在了LF.H中（可以参考论文“Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras” ）；两个平面的距离存储在LF.f, 单位 [mm]； 相机焦距：LF.parameters.intrinsics.focal_length_mm. 生成点云（Point Cloud）接下来我以additional文件下的antinous为例子展示如何利用深度图像（官方利用视差）与纹理图像生成点云。 12345678910111213141516171819202122232425262728filename='antinous';addpath('lib');% 得到antinous的LF.matconvertBlenderTo5D(['additional/',filename])load(['additional/',filename,'/LF.mat']);img=LF.LF(5,5,:,:,:); %中心视角，用于着色r=img(:,:,1);g=img(:,:,2);b=img(:,:,3);% 深度图读取d=pfmread(['additional_depth_disp_all_views\',filename,'\gt_disp_lowres_Cam025.pfm']);d=mat2gray(d);mkdir(['PointClouds-color/',filename]);建立一个文件夹存储图片[ X,Y,Z ] = getPointcloud(LF,'disp',d);ptCloud1 = pointCloud([X(:),Y(:),Z(:)],'color',[r(:) g(:) b(:)]);h1=figure(1);pcshow(ptCloud1);axis offset(gcf,'color',[1 1 1])set(gcf,'Position',[800,300,600,600], 'color','w')view(90.6338, 88.5605);zoom(1.2) 结果如下所示： 注意：生成点云这一步，低版本的MATLAB（如R2014a）由于没有加入相应的函数所以不能够生成点云，高版本（R2016b）可以正常生成。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实习季到了，大家又浮躁了起来]]></title>
      <url>%2F2017%2F04%2F16%2F%E5%AE%9E%E4%B9%A0%2F</url>
      <content type="text"><![CDATA[实习季堪比就业季，今年的形势尤其严峻。伴随着忐忑的心情，我迎来了这个不得不面对的时期。 阿里巴巴视觉算法工程师算法与视觉部分 BF，NN的区别 激活函数的种类 怎么防止过拟合 CUDA的内存模型 HMM是什么 SVM的优缺点 SVD分解的过程 PCA过程 光流法 模版匹配SSD与NCC的优缺点 有哪些形态学操作 相机畸变的参数到底有哪些 交叉熵的概念 Sift与Surf的区别 由前序遍历与中序遍历求后序遍历 深度优先遍历可能的顺序 腾讯基础研究实习生 上机考试包括很多数学方面的知识，比考研数学简单多了，但是范围很广，我想过了这么久大家都忘记了吧。概率论部分占了不少题目，尤其要注意后验概率以及假设检验的题目。基础研究没有编程题目！ 数学部分 特征值与特征向量：Ax=λx 假设检验，第一类错误与第二类错误 求解偏导数 切比雪夫不等式 F分布的性质 简答部分 假设检验来确定零件是否符合标准（可以查看概率论的部分例题） 神经网络以及SVM的对比，优缺点介绍 根据某项调查研究，来确定某结论的正确性； 现场面试部分一面主要包括以下部分： 自我介绍（1分钟内） 项目经历（占了60%时间） 编程题目（反转链表，可参考《剑指offer》） 意向，做工程还是做算法(ps: 被腾讯挂掉了，惨啊) 一面（数据挖掘）猝不及防地又来了一波电话面试，我一脸懵逼的节奏，完全没有准备。我是小白有没有，面试官主要问了以下几个问题： 解释方差，协方差以及样本方差的概念 解释过拟合以及过拟合的概念以及预防措施 解释TCP滑动窗口的概念（这是啥？） 求超级长数组的中位数 析构函数是否可以为虚函数（我是C++小白） 项目介绍 商汤算法实习生 在线笔试 本人申请的岗位是见习算法研究员，笔试1个小时，20道选择填空题，3道编程题。时间略紧。涉及面也非常广，数学，智力，概率统计，线代矩阵，图形学，机器学习，神经网络，C++，均有涉及。 一、选择填空题(部分)： S市A，B共有两个区，人口比例为3：5，据历史统计A的犯罪率为0.01%，B区为0.015%，现有一起新案件发生在S市，那么案件发生在A区的可能性有多大? (概率题，考查贝叶斯公式，牛客网有) A = [1, 2 ; 2, 1]，求A的k次方。(线代，对A进行对角化，求特征值以及特征方程) git常用命令，克隆到本地是（），提交到仓库区（），取回远程仓库的变化，并与本地分支合并（）, 推送所有分支到远程仓库（），显示有变更的文件（）(答：clone, commit, pull, push, status) 表示矩阵需要多少个数字，表示矩阵的投影需要多少个数字? 给出先序序列，中序序列，求后序序列。 一个关于继承和虚函数问题。 掷两个骰子，得到两个数字A,B，设 C = A+B，那么设 C 除以4 的余数为0，1，2，3的概率分别为p0, p1, p2, p3，求它们的大小关系。 图片分辨率为512x512，pad = 2, stride = 3, kernel_size = 9, group = 4, 求卷积后输出分辨率大小。 一个关于图形自由度的问题。（本人完全没概念，所以题目具体记不清了） 以下哪个不能使用迭代器？a) map, b) set, c) queue, d) vector. (c) 有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是() （答案猛击这里） 在一个无序数组中，求前k个最小数字，复杂度最小为？ 根据以下程序：求func(500)的值。(经典问题，相当于求500的二进制中1的个数，《剑指offer》) 12345678910int func(x)&#123; int countx = 0; while(x) &#123; countx ++; x = x&amp;(x-1); &#125; return countx;&#125; 在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）a) 增加训练集量b) 减少神经网络隐藏层节点数c) 删除稀疏的特征d) SVM算法中使用高斯核/RBF核代替线性核 关于vector初始化的一个问题。 有4个车站连通情况如下，每辆车每天都会等概率随机从一个车站出发，然后在某个车站呆一夜，第二天再出发。求稳定之后，每个车站的车辆比例。( 根据马尔科夫链 平稳分布，π=πP( P为转移概率矩阵)，和π1+π2+π3+π4=1，同时π1=π4, π2 = π3。可以求得2:3:3:2 ） 二、编程题: 连续子数组的最大和。（leetcode或剑指offer原题） Minimum Window Substring .(leetcode 原题) 特别鸣谢smallpum123的商汤回忆版！ 现场面试 4月25日，从学校匆匆到了商汤科技进行面试，幸亏提前到了1个小时，要不然就被淋成落汤鸡了。 一共有两个面试官依次面了我，这两名面试官的侧重点不同，第一位是大体了解面试对象，第二位面试官更加具体深入了解面试者。 第一个面试官简单地聊了一段时间，首先是自我介绍，然后是项目经历，最常用的编程语言（我说的是Matlab），然后又问了我有没有用过Matlab的高级函数（bsxfun、ismember等），其他的没有很深入地讨论项目细节。（20 min） 接下来就是第二个面试官，还是重复前面的问题，自我介绍，项目经历，不过这次更加具体了。因为我的方向是做一些基于光场相机的深度图像估计研究的，面试官就问了我关于光场相机原理以及深度估计算法细节方面的东西；然后又问了我第二个基于GPU加速的项目，具体是如何加速代码的（该项目偏工程，没有具体展开）。项目的最后又问了我这些工程的代码量有多大，多少行的样子（我说最少得两、三千行吧）。 最后就是编程题目，面试官问我关于商汤在线评测代码书写的问题，我的回答是：对于连续子数组的最大和问题仅仅写了思路，没有写全代码。然后就是让我现场手写代码了，大概修改了4遍的样子，终于“调试”（所谓调试就是，现场测试代码一步一步写出结果）成功。（60 min） 当然，面试的最后通常面试官都会问面试者想要了解公司的事情，我就如实将我想要知道的事情想他请假了一下下，然后就没有然后了… 经过大概一个半小时，面试结束。无论结果如何，我的心情瞬间轻松许多。还是静候佳音吧~ 电话面试由香港那边的负责人对我进行了远程电话面试，主要包括自我介绍以及项目介绍，重点在项目介绍上面。Ps：当时电话那头是两位面试官听着我的陈述，我竟然浑然不知。就这样过了40分钟，结束。等待的时间最为忐忑，我觉得自己表现平平，不知道给面试官留下了什么印象。 顺利通过经过一个漫长的劳动节并时逢校庆的假期，5月6号的下午收到了HR打来的电话，成功通过面试，现在心情还是特别激动。 搜狐图像处理实习生 初选简历过关之后进行面试。 笔试（60min）根据应聘的实习岗做不同的题目，因为我面的是千帆直播下的图像处理岗位，所以我的题目中有很多关于这方面的相关知识。以下是我的会议版本： do while 和while do 的区别 char const *p 与char * const p 的区别(答：p都是指向const char类型的指针, 不可以赋值给*p, 就是不可通过这个指针改变它指向的值; 第二个: char * const p是指向char的常指针, 指针需在声明时就初始化, 之后不可以改变它的指向) 创建并且初始化一个双向链表 代码实现二分查找 对一个WAV格式的文件头用适当的数据结构进行表示 队列与栈的区别，分别以什么数据结构表示 常见的视频压缩方法，视频格式，音频格式 汇编语言和C/C++混合编程有哪些方法 如何引用一个已经定义好的全局变量，并比较异同 gdb如何调试线程，多线程呢（ps:我根本不会这个题目） 解释“熵”的概念（答：熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。信息熵表示信息的丰富程度，定义为E=-plog(p)） 请解释1080p的含义（答：1080指的是分辨率1920*1080，p为扫描方式：逐行扫描） 请解释FPS的全称以及含义（答：Frames Per Second，帧率的意思） 解释“码率”的概念（答：即比特率，一秒钟处理的数据量大小，影响到视频的质量以及帧率） MPEG标准中有哪些帧类型 有以下数据结构，请问最后输出结果的是？（注意共用体的大小）12345678910111213typedef union&#123; long i; int j[3]; char k;&#125;DATA;struct data&#123; int m; DATA n; double q;&#125;;data max;printf("%d/n",&amp;(sizeof(DATA)+sizeof(max)); 还有其他的题目，记不太清了，但是主要就是以上的。难度适中，即有涉及到程序也有图像以及视频处理的相关知识。因为当时没有好好准备，猝不及防的给我了这些题目，感觉一脸懵逼。 现场面试面试小哥很nice，人很好。首先是自我介绍，然后就是项目经历。基本上简历上的内容问了一遍，感觉还不错。问了我如果调试正在进行中的程序，如何用markdown语言引用代码，对Latex的了解等等。小哥面试结束后，以为女面试官姐姐再次对我的一些基本情况进行了询问，最后还送了一个可爱的小狐狸。无论结果如何等结果吧，祝我好运！ps：很幸运地被录为实习生，但是还是选择了商汤。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[中国计算机学会推荐国际会议及期刊目录]]></title>
      <url>%2F2017%2F04%2F11%2F%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%BC%9A%E6%8E%A8%E8%8D%90%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E5%8F%8A%E6%9C%9F%E5%88%8A%E7%9B%AE%E5%BD%95%2F</url>
      <content type="text"><![CDATA[CCF会议列表 顺便听首歌吧嘿嘿~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[日本与美国之行]]></title>
      <url>%2F2017%2F03%2F19%2F%E6%97%A5%E6%9C%AC%E4%B8%8E%E7%BE%8E%E5%9B%BD%E4%B9%8B%E8%A1%8C%2F</url>
      <content type="text"><![CDATA[日本国之行 小时候因为极其喜欢动漫的缘故，我喜欢上了日本，从那时起已经向往能够进行一次日本之行。2017年1月8日早上的闹钟比以往想得更加早些。迎着黎明前第一缕阳光的到来，我们来到了香港机场。即将开始人生第一次出国高校访问之旅，我心情相当激动。 飞机经历两个小时到达日本，偌大的东京比深圳多了一份凉意。拖着行李坐着日本地铁匆匆来到了池袋的旅馆，安顿下来，第一天就这样结束。接下来的几天我们团队依次到了东京、京都、奈良等地，参观日本几所具有代表性的大学，体验日本的文化以及感受日本的风土人情。 初来乍到，凛冬已至，虽寒意袭来，但心中激动之情让我变得火热。日本之行我们团队参观了5所大学（早稻田大学，东京大学，法政大学，京都大学，秋田大学）以及1个科研院所（NHK科技研究实验室）。 日本同学介绍的科研内容并非高大上的东西，有些只是利用现有的算法来解决现实问题；但是他们做出的东西都是系统化的，最终的结果是以实物或者可视化的形式作为展示。具体而言，对于早稻田大学3个机器人实验室，他们的研究方向更加贴近于实际生活，而并非空谈理论，关注于“人”本身，而并非突发奇想。一位早稻田大学的师兄提到，他们的研究成果必须最终以Demo的形式展示，相比之下，国内科研更加注重仿真，如若计算机仿真能够成功的实现预期目标算是大功告成，省去了硬件实现或者可视化展示这一步。这对于该领域内的科研人员而言是及其容易理解的，但是对于大众而言，大都不能够理解算法的原理以及实现细节，他们关注的是研究的目的以及能够解决的问题。因此如何以一种通俗易懂的方式向大众介绍自己的研究内容成为了一个问题，日本高校相比国内高校在科研成果展示方面具有一定的领先。 值得注意的是，最后一站的秋田大学与以前的几个大学有着明显差异，它位于日本的北端，抵达时正值秋田几十年来最大的一场雪，这场大雪增加了参观秋田大学的乐趣。起初，我们对于这个农业科学见长的高校知之甚少。通过对其参观，我了解到了这个新兴高校的发展历史以及现有科研水平，秋田大学如何在这十几年时间内迅速成长必然值得我国高校学习。 总结而言，东京大学与京都大学历史悠久，科研气氛浓厚且资源丰厚；早稻田大学注重实践，校企合作成为常态；政法大学兼容并包，面向世界；秋田大学后起之秀，努力吸收优秀资源。尽管参观时间有限，但是我们能够明显的感觉到日方高校对于我们来访的热情，同时也可以了解到日本最有代表性大学的科研水平。最大的收获在于，亲身感受到了跨文化交流。我们用英文与日本同学交流，语言不再是最大的障碍。保持好奇心一直是我的座右铭，尽管研究领域不尽一致，但我会努力的提出自己的疑问并聆听日本同学的讲解。起初时候会担心自己不能够很好的提出疑问或者不能够听懂他们的回答，但是主动尝试之后会发现，自己所担心的问题并不存在，同时我也逐渐喜欢上了与他们之间的交流的过程。 京都游玩的地方很多，交流之余，游遍几个好玩的地方 此外，很早就听说日本民众素质很高，这次真切的感受到了。值得注意的是，他们在日常生活中表现出的礼貌也值得人敬佩，这很符合日本人不愿给别人添麻烦的特点。他们时常把“すみませ”挂在嘴边，比如进电梯时，进入餐厅时或者超市付款时；无论多赶时间也会排队，另外在公共场合他们不会大声喧哗，具体而言在地铁上无论人再多再挤，车厢内也是安静的让人感觉不可思议；他们的房屋建设的很低，密密麻麻的挤在一起，电线在房屋之间穿梭，但是让人感觉并不杂乱，“规矩”一词用在他们身上并不为过。同时日本对于古老传统的保留非常重视，以前就听说日本是保留中国古文化最好的非华人国家。果然，“相扑，茶道，和服，空手道”等诸多优良文化在日本保留的相当好，在日剧以及日本民族产业动画中都有很好体现。改革开放以来，我国的经济实力突飞猛进，如今已经跃居世界第二大经济体，但是国际上的文化软实力远远不如日本。很多欧美人士所了解的很多“日本传统”实际是源于中国，日本在国际上的形象是优于中国的。“择其善者而从之”，我认为国内的道德建设以及文化建设必须同经济发展相协调，提高全民素质，打造真正属于中国的国际形象。 美利坚合众国之行 To be continued…先上图，占个坑]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Light Field 光场以及MATLAB光场工具包(LightField ToolBox)的使用说明]]></title>
      <url>%2F2017%2F02%2F16%2FLightField%E5%85%89%E5%9C%BA%2F</url>
      <content type="text"><![CDATA[这里我汇总了有关光场（Light Field）一些有用的链接以及光场数据的处理过程。目前还在整理中，随时更新。声明：一切理解都是本人观点，如有疑问，还望在评论中留言。如需转载请与本人联系，谢谢合作! 邮箱：点我 光场相机大家在刚刚入手光场领域的时候会用到现在的消费者用的手持光场相机Lytro或者ILLUM，如图（实验室的设备，我可买不起ILLUM）： 获取.LFP(or .LFR)原文件由Lytro拍摄的图像的原格式是.lfp格式，我们要解码成我们需要的格式。工具：Lytro Desktop，MATLAB光场工具包（很强大，推荐，本文介绍该方法）。用数据线把设备连接到电脑，打开Lytro Desktop，点击想要导入的图片，选中点击右上角立即处理，然后打开我的电脑图片-&gt;Lytro Desktop\Libraries\Lytro 图库.lytrolibrary*，就可以发现有很多文件夹名字是一串很长数字字母啥的。点击去就可以发现好几个文件，以lytro为例，这几个文件如下形式： raw.lfp就是我们需要的原文件，之后我们就要利用Matlab光场工具包对其进行解码操作。 Matlab Light Field ToolBox(光场工具箱)的使用下载光场工具包（LFToolBox）首先下载光场工具箱并仔细阅读说明文档，根据文档把相应的数据拷贝到工具箱的文件夹下(这一步很关键，要仔细配置)。如果不想在官网下载的话我上传到了度娘的云盘链接：链接 密码：yykc。这是我修改后的一个版本，可以直接运行，推荐下载这个版本。另外我在Github上传了一个版本，大家可以git clone链接。下载下来的工具包是这样的： LFToolbox0.4就是我们要的工具包，里面包含很多函数，如下图： 我把一些比较常用的函数或者文档用红色的框标注出来，其中PDF文档是该工具包的说明书。这个说明书中详细的介绍了该工具包的使用方法，我们完全可以根据该文档的介绍来实现自己想要的功能。如下是该说明书的截图： 同时该说明文档提供了各种函数用于从LFP文件中提取出自己需要的各种信息：白图像(white image)，Raw Image，对齐后的图像，以及颜色校正，频域滤波后的图像等。因为时间不足没有整理的，感觉大家都对这个过程比较感兴趣，我觉得有必要写一下到底如何读取lfp、lfr、raw文件了。好了言归正传，开写。 前期准备step 1: 创建自己的工作目录如果是直接clone我在github上的工程的话直接跳转step 2。如果没有，那就要建立自己的工作目录，便于文件的管理。这一步是必要的，如果建立的目录不一致，可能导致程序无法运行，这也是我当时初次用这个工具包时常常出错的地方。好了，建立这样的目录结构： step 2: 根据相机序列号修改文件名Sample_test表示我们的测试目录，里面包含了相机信息以及自己拍摄照片的图像（lfp/lfr）。Cameras 这个目录中又包含了几个文件夹，它们分别是以“A”或者“B”开头，在其后面有一长串数字。这其实就是光场相机的serial number，我们可以从默认目录 c:\Users\\AppData\Local\Lytro\cameras\sn- serial_number中找到，这个数字每个相机不一样，大家根据自己的相机序列号修改这个目录哈。”A”表示的是LYTRO系列相机，“B”表示ILLUM系列相机；以上图为例，”A303134427”就是我相机的序列号。 step 3: 把白图像文件拷贝到相应的文件夹下在每个序列号文件夹下又有一个文件夹WhiteImages，这里面放着由该相机拍摄的白图像。所谓白图像就是一张由光场相机拍摄的白色的图像，当然自己也可以拿着光场相机对着白色的墙面拍几张，但是效果并不一定很好。庆幸的是LYTRO官方提供了白图像，以Lytro为例，我们可以从以下目录找到: c:\Users\ username\AppData\Local\Lytro\cameras\sn- serial_number。如下图所示： 我们发现这里有以下4个文件：data.C.0/1/2/3，这是官方把图像压缩成了这种格式，我们需要用工具箱进行解码。我们需要的就是这四个文件，拷贝出这4个文件，放在WhiteImages文件夹里。这一步相当关键，一定要确保拷贝对了目录。注意，Illum相机的白图像与Lytro的白图像的存放位置不一样，在相机的SD卡里。 step 4: 将测试文件放到Images目录Images文件夹下包含我们需要处理的文件们，F01下存放LYTRO系列拍摄的文件，B01下存放ILLUM系列拍摄的文件。以Lytro为例，由于前面已经有了测试文件raw.lfp，我们就把这个文件放在F01下。经过我们上述的过程之后，最后我们的目录会变成这样（注意：Sample_test与LFToolBox0.4为同级目录，各个文件夹的名字务必正确）： 测试开始我用的是实验室的电脑，配置是：Intel(R) Core(TM) i7-4790 CPU @3.60GHz 3.60GHz RAM 16GB。其中的Demo文件是本人编写的一小段测试代码，已经贴在了文末。接下来的过程就是RUN CODE了。程序大致可以分为以下几个测试： 处理白图像 解码LFP文件 频域滤波 颜色校正 处理白图像处理白图像的目的为了得到相机的某些参数，我当时是为了获得每幅光场的中心点坐标才进行的这一步。以为白图像拍摄的场景没有纹理，此时可以清楚的观察到微透镜成像的边界信息。如下图所示： 可以看到的是，微透镜下成像是这种正六边形的网格，类似于蜂窝的结构，感觉666。需要注意的是，该过程不是简单的提取出一张白图像来，而是提取几十张白图像对（image pairs），这个过程运行起来有点久，以下是运行的截图： 解码LFP文件如果只是单纯的读出LFP/LFR、RAW文件的数据的话可以分别用工具包提供的如下函数：LFReadLFP、LFReadRaw。注意两个函数的返回值不一样。LFReadLFP返回一个结构体类型的变量，它包含相机的各个信息，我们可以根据自己的需要保留数据。LFReadRaw返回的是一张uint16的灰度图，还没有经过demosaicing操作。去马赛克操作在malatb中有相应的函数，这点不用担心。我们在这里不是直接调用的LFReadLFP而是调用了工具箱提供的LFLytroDecodeImage函数。如果运行有问题（若是直接clone我github上项目的话，不需要修改），将LFLytroDecodeImage中的WhiteImageDatabase路径由以下： 12DecodeOptions = LFDefaultField( 'DecodeOptions', 'WhiteImageDatabasePath'...,fullfile('Cameras','WhiteImageDatabase.mat'));% line 71 改为： 123DecodeOptions = LFDefaultField( 'DecodeOptions', 'WhiteImageDatabasePath'... ,fullfile('Cameras',LFMetadata.SerialData.camera.serialNumber,'WhiteImageDatabase.mat'));%== 注意，这条插在 ---Select appropriate white image---上行，而不是在原来的71行修改==。 经过这样的修改之后，这下应该可以跑了。我们可以得到以下图像： 局部放大效果图： 所有视角的图像： 这时候可以看到在边界视角上的图像比较黑，所以我们接下来要进行频域滤波，以及颜色校正。 频域滤波以及颜色校正这部分分别用到了LFFilt4DFFT以及LFColourCorrect函数。以LYTRO 1.0 为例子，我们得到的光场图像一种有11*11个视角，但是这个121个视角子孔径图像的质量真的不敢恭维，尤其是边角处的视角(u=1,v=1)时，这个图像时完全变成黑色的。所以嘛，LFFilt4DFFT这个函数是将这些变成黑色的图像或者质量不好的图像进行校正的，具体原理不作展开。LFColourCorrect这个函数是利用gamma变化对原始图像进行颜色校正的，这一点比较简单。总之利用这两个函数能够让我们得到的光场图像的质量更好，当然你也可以选择不用。 以下是经过滤波之后的所有子孔径图像，可以发现边界的视角相比于频域滤波之前有了很好的可视性。 以下是经过颜色校正之后的所有所有子孔径图像。 经过以上的步骤我们可以学习到白图像的处理，以及光场图像的处理等操作。当然我没有列出这个工具包所有的功能介绍，大家可以根据需要建立自己工程，对自己的数据进行测试，以上！ 注意： 务必在WhiteImagesPath处写明相机型号，确定好到底是Lytro还是Illum 注意Illum相机的白图像们在相机的SD卡中，那些白图像们拷贝出来放在路径Sample_test\Cameras\B5151500510\下即可 白图像的处理过程比较久，耐心等待就行即可 Lytro与Illum的频域滤波调用函数是不同的，我已经把代码添加在了相应位置；这个函数用时较久，耐心等待 结果存放在Results_saving文件夹下 再次提醒，由于Illum图像的分辨率比较大，所以当程序运行到LFLytroDecodeImage以及频域滤波时会造成内存以及磁盘的大量使用，慎重考虑。 如有Bug请及时联系我，请在评论区留言。 以下是Demo文件的代码，仅供学习使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112clc;clear all;clc;addpath(genpath('Sample_test'));addpath(genpath('LFToolbox0.4'));LFMatlabPathSetup;%% Step1: 解压data.C.0/1/2/3---&gt;white,结果存储在Cameras中fprintf('===============Step1: Unpack Lytro Files===============\n\n ');LFUtilUnpackLytroArchive('Sample_test')%% Step2: 包含刚刚解压出来的文件的目录fprintf('===============Step2: Process WhiteImages===============\n\n');WhiteImagesPath='Sample_test\Cameras\B5151500510'; % 务必要设置这个值 B5151500510 A303134427LFUtilProcessWhiteImages( WhiteImagesPath);%% Step3: 解码光场图像.lfpfprintf('=====================Step3: Decode LFP===================\n\n');cd('Sample_test'); % 进入Sample_test目录lfpname='baby'; %测试图像名称，改成你自己的if WhiteImagesPath(21)=='A' %找到型号 exist('LYTRO','var') version='F01';elseif WhiteImagesPath(21)=='B'%找到型号 exist('ILLUM','var') version='B01';endInputFname=['Images\',version,'\',lfpname,'.lfp'];[LF, LFMetadata,WhiteImage,CorrectedLensletImage, ...WhiteImageMetadata, LensletGridModel, DecodeOptions]... = LFLytroDecodeImage(InputFname);cd('..');imshow(CorrectedLensletImage) %Raw Imagemkdir(['Results_saving\',lfpname]);imwrite(CorrectedLensletImage,['Results_saving\',lfpname,'\',lfpname,'.bmp']);save(['Results_saving\',lfpname,'\',lfpname,'.mat'],'CorrectedLensletImage');%% =======================频域滤波================================%---Setup for linear filters---tic% lytroif strcmp(version,'F01')==1 LFPaddedSize = [16, 16, 400, 400]; BW = 0.03; FiltOptions = []; FiltOptions.Rolloff = 'Butter'; Slope1 = -3/9; % Lorikeet Slope2 = 4/9; % Building fprintf('Building 4D frequency hyperfan... '); [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions ); fprintf('Applying filter'); [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut );% illumelseif strcmp(version,'B01')==1 LFSize = size(LF); LFPaddedSize = LFSize; BW = 0.04; FiltOptions = []; %---Demonstrate 4D Hyperfan filter--- Slope1 = -4/15; % Lorikeet Slope2 = 15/15; % Far background fprintf('Building 4D frequency hyperfan... '); [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions ); fprintf('Applying filter'); [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut ); title(sprintf('Frequency hyperfan filter, slopes %.3g, %.3g, BW %.3g', Slope1, Slope2, BW)); drawnow save(['Results_saving\',lfpname,'\',lfpname,'5D.mat'],'LFFilt');end%% =======================颜色校正参数设置========================== ColMatrix = DecodeOptions.ColourMatrix;Gamma=DecodeOptions.Gamma;ColBalance=DecodeOptions.ColourBalance;% 对3280*3280的原始彩色图像进行颜色校正ColorCorrectedImage=LFColourCorrect(CorrectedLensletImage, ColMatrix, ColBalance, Gamma);imwrite(ColorCorrectedImage,['Results_saving\',lfpname,'\',lfpname,'ColorCorrectedImage.bmp']);save(['Results_saving\',lfpname,'\',lfpname,'ColorCorrectedImage.mat'],'ColorCorrectedImage')imshow(ColorCorrectedImage);title('Corrected Lenslet Image');%% 同样是颜色矫正， 为了得到光场数据。得到5-D LFColorCorrectedImage数据LFColorCorrectedImage=zeros(size(LF,1),size(LF,2),size(LF,3),size(LF,4),size(LF,5));for i=1:size(LF,1) for j=1:size(LF,2) temp =squeeze(LFFilt(i,j,:,:,1:3)); temp = LFColourCorrect(temp, ColMatrix, ColBalance, Gamma); LFColorCorrectedImage(i,j,:,:,1:3)=temp; imshow(temp); pause(0.1) endendLFColorCorrectedImage(:,:,:,:,4)=LF(:,:,:,:,4);save(['Results_saving\',lfpname,'\',lfpname,'RawLFColorCorrectedImage.mat'],'LFColorCorrectedImage');% very importanttoc%-------------------------------------------------------------------------------- ## Lytro官网可视化工具 谁让人家Lytro不开源呢，人家自己做的Demo还不错。通过鼠标就可以对以下图像进行重聚焦，变化视角，以及缩放等操作。话不多说，上图！ 呵，人家公司在2017年11月30号之后停止了对lytro live photo的线上支持，所以以下啥都没有了。 日常来首歌曲：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matlab相关问题汇总]]></title>
      <url>%2F2017%2F01%2F16%2FMatlab%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
      <content type="text"><![CDATA[以下是我在使用Matlab编程时遇到的问题以及解决方法，最后彩蛋随时补充。 Matlab 写入Excel错误问题描述 Matlab 在创建EXCEL文件的时候总是出错，即使使用MATLAB自带的程序。 问题描述：在Matlab中使用xlswrite函数时，如果excel文件存在时，则程序能够正常运行；当excel文件不存在时，则会出现错误： 1Error using xlswrite (line 220) Error: 服务器出现意外情况。 问题解决：xlswrite函数在调用时会占用Excel的com端口，所以要保证在调用时这个端口是开放的，也就是没有被其他程序占用。打开任意一个Excel（我的是16版）文档，点击文件—选项，弹出Excel选项卡，在加载项中可以看到，活动应用程序加载项，以及非活动应用程序加载项；由于我的系统中装了一个福昕阅读器，该程序占用了Excel的com端口，所以当Matlab再去调用这个端口时就会出现异常。具体解决方法：点击管理旁边的下拉菜单，选择COM加载项，点击转到，把福昕阅读器的前面的勾去掉，然后确定。![这里写图片描述](http://img.blog.csdn.net/20160628202102846)![这里写图片描述](http://img.blog.csdn.net/20160628202117268) Matlab设置绘图坐标轴信息问题描述 Matlab 作图时更改纵轴刻度为科学计数法，指数放在框左上方 12345678plot([0 1],[0 .02]) % 作图，换成自己的图像就可以~oldLabels = str2num(get(gca,'YTickLabel'));scale = 10^2;newLabels = num2str(oldLabels*scale);set(gca,'YTickLabel',newLabels,'units','normalized');posAxes = get(gca,'position');textBox = annotation('textbox','linestyle','none','string',['x 10\it^&#123;' sprintf('%d',log10(1./scale)) '&#125;']);posAn = get(textBox,'position');set(textBox,'position',[posAxes(1) posAxes(2)+posAxes(4) posAn(3) posAn(4)],'VerticalAlignment','cap'); 今天学到一个特别简单的语句，删除元胞数组中空元素： a(cellfun(@isempty,a))=[]；（2016.05.19） Matlab显示图片错误问题描述 MATLAB图像显示总是白色 imshow是一个很强大的”武器”，显示图像简直无所不能，但这其中往往会出现问题。在处理图像时，我们的图像经常是经过了某种运算，为了保证其精度，系统会自动的将uint8型数据类型转化成double型。 “如果直接运行imshow(I)，我们会发现显示的是一个白色的图像。这是因为imshow()显示图像时对double型是认为在0~1范围内，即大于1时都是显示为白色，而imshow显示uint8型时是0~255范围。而经过运算的范围在0-255之间的double型数据就被不正常得显示为白色图像了。 ” 解决之道 可以利用mat2gray()函数，这个函数是归一化函数，可以把数据归一化到0-1之间，再用imshow()就可以了； 或者对于一个处理后的黑白图像Img，若为double型可以这样写：imshow(Img/max(Img(:)))； 还有一种就是：imshow(Img,[]);就是加一个[]，即可以自动调整显示； 彩蛋 大神教我们怎么画图，#MATLAB无所不能#，戳戳这里~ Matlab 字体困扰了我很长时间，终于在网上找到了一个比较好的组合，猛戳这里！原文地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HEXO建站备忘录]]></title>
      <url>%2F2016%2F08%2F09%2FHEXO%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98%E5%BD%95%2F</url>
      <content type="text"><![CDATA[Hexo作为建立Blog利器，为我们没有JS基础的小白们提供了建立专属自己博客的机会！经常使用的语法很简单，我们完全可以在10min分钟之内建立自己的Blog，后期的优化才是最耗费时间的。好了，直接进入正文。 123456hexo clean # 清除缓存，简写 hexo chexo generate # 作用：建立静态页面， 简写 hexo g hexo deploy # 部署自己的blog，本人部署在了Git上，简写 hexo dhexo server # 以启动本地服务， 可预览，简写 hexo shexo new blog_name # 新建以blog_name为名的blog在.md文档中加入 &lt;!-- more --&gt; 可以显示“阅读全文” 显示文章阅读数量另外：显示文章阅读量， 服务主要用了LeanCloud服务提供商 主题设置Make the theme more beautiful, recommended Plus: 我使用的是Next主题 关于多说多说已死，评论系统转到了Disqus，但是被墙的事实让人感觉不爽。几经周折，从多说转到Disqus，然后在gitment和gitalk之间徘徊，最后还是选择了valine，不过它只能在中国区进行评论，于是我还是保留了gitalk。于是我总结出来我属于爱折腾的那种人。 关于旋转头像把侧边栏头像变成圆形&amp;鼠标停留在上面出现旋转效果，具体修改文件的位置是next\source\css\_common\components\sidebar\sidebar-author.styl。更为具体的修改过程见Ehlxr写的这篇博客。1234567891011121314151617181920212223242526272829303132.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ -webkit-animation: play 2s ease-out 1s 1; -moz-animation: play 2s ease-out 1s 1; animation: play 2s ease-out 1s 1; /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.5s ease-out; -moz-transition: -moz-transform 1.5s ease-out; transition: transform 1.5s ease-out;&#125;img:hover &#123; /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;```language 背景颜色设置其实NEXT主题已经自带了几种动画了，我用的是three_waves；但是呢，存在一个问题就是因为Blog背景是透明的，这样文字和背景动画就有重叠效果了，很不方便阅读，这时把背景色设置为白色即可。添加background: white到如下路径next\source\css\_schemes\Muse\_layout.styl1234.header-inner, .container .main-inner, .footer-inner &#123; background: white; +mobile() &#123; width: auto; &#125;&#125; 页面宽度设置有时候我们可能会嫌弃博客的页面太小，留白过大。这时候可以对页面宽度进行设置，可以参考Hexo Next主题 Issue #759。对于 Pisces Scheme，需要同时修改 header 的宽度、.main-inner 的宽度以及 .content-wrap 的宽度。例如，使用百分比（Pisces 的布局定义在 source/css/_schemes/Picses/_layout.styl 中）： 123.header&#123; width: 80%; &#125; /* 80% */.container .main-inner &#123; width: 80%; &#125; /* 80% */.content-wrap &#123; width: calc(100% - 260px); &#125; 优化友情链接新建一个Friends页面：1hexo new page Friends 新建样式，进入E:\Blog-new-gitment-new-theme\themes\next\source\css\_custom\custom.styl，在最后新加上几行代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061$shadowColor = #333;$themeColor = #222;$link-image-size = 180px;.link-body&#123; ul&#123; display: flex; justify-content: space-between; align-items: center; flex-wrap: wrap; margin: 0; padding: 0; .link&#123; max-width: $link-image-size; min-width: $link-image-size; max-height: $link-image-size; min-height: $link-image-size; position: relative; box-shadow: 0 0 1px $shadowColor; magin: 6px; width: 20%; list-style: none!important; overflow: hidden; border-radius: 6px; img&#123; object-fit: cover; transition: transform .6s ease-out; vertical-align: middle; border-bottom: 4px solid #eee;//#e5642b; transition: 0.4s ; width: 100%; border-radius: 90px 90px 90px 90px; display: inline-block; float: none; vertical-align: middle; &#125; .link-name&#123; position: absolute; bottom: 0; width: 100%; color: #666; text-align: center; text-shadow: 0 0 1px rgba(0,0,0,.4); background: rgba(255,255,255,.7); &#125; &amp;:hover&#123; img&#123; overflow: hidden; //transition: 0.4s; border-radius: 0 0 0 0; &#125; .link-name&#123; color: $themeColor; text-shadow: 0 0 1px $themeColor; &#125; &#125; &#125; &#125;&#125; 然后编辑站点的source\Friends下的index.md文件，内容如下： 12345678910111213141516&lt;div class="link-body"&gt; &lt;ul&gt; &lt;!--your friend begin--&gt; &lt;li class="link"&gt;&lt;a href="your_friends_link" title="balabala" target="_blank" &gt; &lt;img src= "image_path" alt="balabala"/&gt; &lt;span class="link-name"&gt; balabala&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;!--your friend end--&gt; &lt;!--your another friend begin--&gt; &lt;li class="link"&gt;&lt;a href="your_friends_link" title="balabala" target="_blank" &gt; &lt;img src= "image_path" alt="balabala"/&gt; &lt;span class="link-name"&gt; balabala&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;!--your another friend end--&gt; ... &lt;/ul&gt;&lt;/div&gt; MarkDown编辑器推荐Haroopad 插入PDF文档以及图片 插入PDF文档：将相应的PDF文档放在与博客标题同名的文件夹内，然后再按照如下方式进行插入。 1[点我，这里是PDF文档](latex入门教程.pdf) 点我，这里是PDF文档 利用js嵌入图片 12&lt;img src= image_path alt="Lytro相机" width="100%"&gt;&lt;center&gt;Lytro&lt;/center&gt; 注意以上的image_path既可以是图床中的路径，亦可以把图片放在source/images/文件下，然后image_path=/images/logo.png，当然也可以如下插入图片，更加方便。 1![](/images/logo.png) 利用插件，以下我在Github上找到的别人已经做好的一个小工具。 安装 hexo-tag-asset-res 打开Git Shell, 在Hexo根目录下, 输入如下代码 1$ npm install hexo-tag-asset-res --save 修改Hexo根目录下_config.yml文件 打开Hexo根目录, 找到_config.yml文件, 用任何一个文本编辑器打开, 找到如下代码 1post_asset_folder: false 测试插入 配置个性化的字体在next\source\css\_variables\custom.styl文件中添加如下内容。 12345678910// 修改成你期望的字体族$font-family-base = "Monda","Microsoft YaHei", Verdana, sans-serif// 标题，修改成你期望的字体族$font-family-headings = "Roboto Slab", Georgia, sans// 代码字体$code-font-family = "PT Mono", "Input Mono", Consolas, Monaco, Menlo, monospace// 博客字体$font-family-posts = "Monda"// logo字体$font-family-logo = "Lobster Two" 在博客中插入网易云音乐我们可以利用网易云提供的代码直接在markdown文档里面插入。 在网页上找到你想要播放的音乐，如下图： 点击生成外链播放器 注意自动播放，以及音乐播放器的大小可调。 在Markdown文档里插入如下代码 12&lt;center&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=500 height=86 src="http://music.163.com/outchain/player?type=2&amp;id=29722263&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;/center&gt; 同时部署接下来主要涉及到以hexo框架搭建博客的版本管理。同时部署其实很简单，仅仅修改站点配置文件的_config.yml即可。在最后的deploy底下新增一项： 123repo: github: https://github.com/Your_Github_ID/Github_ID.github.io.git coding: https://git.coding.net/Your_Coding_ID/Your_Repo_Name.coding.me.git 以后hexo d时，就会同时部署到github和coding。 版本管理方案 1（推荐）下载第三方插件，more information refers to this link hexo-git-backup. When you are well configured, you can just run the following command.1hexo backup #或 hexo b 方案 2这里涉及到git的部分知识。 目的：实现整个blog源码级别的代码管理，包括站点配置&amp;主题配置。 首先明确一点，在每次hexo d时，都会自动产生一个名为.deploy_git的文件夹，这个文件夹下包含有hexo g渲染出的各种文件，这些文件就是构成github page或者coding page的重要源码；同时会自动的将这个.deploy_git设置成本地仓库，即产生一个.git的隐藏文件。我们做的事情和以上过程不尽一致，总结起来主要是以下几个命令。首先建立一个名为.gitignore的文件，表示我们并不上传这些文件，原因后续介绍。其内容为： 12345678.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/themes/ 接下来就是把blog的源文件夹搞成一个本地仓库，如下命令。 12345678910111213141516171819# 创建仓库git init # 为本地仓库添加文件git add -A# 提交到本地仓库git commit -m "your message"# 添加一个名为 origin 的远程，这个名字随便起git remote add origin https://github.com/Your_Github_ID/Your_Repo_Name.git# 为其添加 push 到 Github 的地址git remote set-url --add --push origin https://github.com/Your_Github_ID/Your_Repo_Name.git# 为其添加 push 到 Coding 的地址git remote set-url --add --push origin https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git# push到远端的master分支git push --set-upstream origin master# 新建并切换分支git checkout -b "another-branch"# 各种更改文件......推送到远程git push --set-upstream origin another-branch# 以后可以直接 git push，不用set了。 通过以上命令，我们就可以同时部署在github仓库https://github.com/Your_Github_ID/Your_Repo_Name.git和coding仓库https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git了。 设置主题远程仓库这时你会发现themes这个文件夹并没有同时被上传到远程仓库，同上操作，将theme/next设置成本地仓库并部署。之所以将这个仓库单独上传，是为了方便切换主题，以及主题升级。 设置node_modules远程仓库之所以将这个模块单独拎出来处理，是因为这个文件夹虽然容量不大，但是其中文件个数特别多。当和blog源文件一同被git add到暂存区之后，git shell的运行速度就会超慢。我的解决思路就是将其创建成一个仓库，这样git shell的速度就会快一些。具体步骤不再赘述，同上。 结语经过建立以上的3个仓库，实现了blog源码级别的版本管理。当然，如果你不想暴露自己的源码，那么你只需要在coding申请一个私有仓库并部署就ok了。虽然看起来有些麻烦，但是一旦配置完毕之后，我们就只需要以下几个步骤就可以实现管理。12345hexo clean # 不是必要步骤hexo d -g # 渲染+部署到github page以及coding pagegit add . # 添加到暂存区git commit -m "your message" # push到本地仓库git push # 上传到远程仓库（站点目录、next主题目录、node_modules目录） Good luck:)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[AR形势与应用]]></title>
      <url>%2F2016%2F08%2F09%2FAR%E5%BD%A2%E5%8A%BF%E4%B8%8E%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[前言当前，微软、谷歌、苹果、Facebook 等 IT 巨头都在布局虚拟现实Virtual Reality简称 VR）虚拟现实也许是下一个颠覆人类生活的新技术之一。增强现实Augmented Reality简称 AR）是虚拟现实技术的延伸它可以用来模拟对象让学习者在现实环境背景中看到虚拟生成的模型对象 而且这一模型可以快速生成、操纵和旋转。 1．增强现实公司分类Augmented reality增强现实创业的公司大致可以分成如下几种公司类型[1]。（以下是翻译的国外网站的内容） AR平台公司（AR platform companies）。这些公司为开发者提供底层的开发工具，以便于开发者能够创造更多更加高级的AR解决方案。这样的公司有：Qualcomm Vuforia, METAIO’s SDK, TotalImmersion。 AR产品和游戏公司。这些公司主营自己独家AR零售产品例如：书籍、游戏。包括如下公司：Sphero, POPAR, Sony, Microsoft 以及 Nintendo。 自助DIY AR公司以及通用AR查看器。这些公司专为快速简单的AR体验或活动而设计，提供内容管理工具和基本AR效果菜单。借助自助AR工具，精通技术的个人可以创建简单的体验，例如发布单个视频或简单的动画。AR自助服务公司非常适合发布商，教育工作者，学生和其他想要测试或创建简单的增强现实体验的用户，而无需投资于完全定制的品牌应用体验。一些DIY公司还提供AR查看器，定制服务和白标签选项。这个领域的公司包括Layar，Aurasma，DAQRI和Zappar。 定制品牌应用开发公司。这些公司直接与品牌营销人员和机构合作，主要为的广告活动、贸易展览和现场活动构建定制的增强现实解决方案。自定义品牌应用程序允许营销人员结合独一无二的定制增强现实体验与个性化服务和项目管理。自定义功能通常包括品牌规格，导航，用户界面，动画，复杂或大规模的AR效果等。服务可以包括3D建模，与其他软件服务或电子商务平台的集成，游戏开发，基于位置的安装，通知，复杂动画，微位置或其他高级AR效果。这个领域的公司包括Appshaker，GravityJack和Marxent。 行业特定的垂直AR解决方案（Industry-specific vertical AR solutions）。最新出现的AR公司类别是那些提供AR解决方案的专业服务公司，专为服务于专业领域。如奢侈品零售，医疗服务，工业应用，制药公司和化妆品公司。这个领域的公司包括用于广告的Blippar，用于豪华珠宝零售的Holition，用于家具布置的Adornably以及用于消费零售，工业和企业销售工具的Marxent的VisualCommerce®。 做个不恰当的比喻，我觉得AR是VR的一个延伸，只是把VR的场景换成了现实场景，眼镜换成了透明的。 2．可行性分析一个良好的AR体验，大致可以分成一下几个方面： 1． 3D眼镜；这应该是VR或者AR最为重要的一部分； 2． 手柄；可以代替人手去操纵，在一定程度上增加了使用的灵活性。（但我觉得未来手柄一定会被淘汰，因为这只是人手不能被充分利用的代替手段）。 3． 3D显示屏幕，它可以跟踪用户的头的转动和手的动作，实时调整所看到的3D图像，并允许用户操控一些虚拟物体，就好比他们真正存在。 4． 待补充 技术难点VR得益于三维游戏的发展，而AR收益于影视领域的跟踪技术（video tracking）的发展。从技术门槛的角度来说，VR、AR和移动端重合的技术有：显示器、运动传感器、处理器、储存、记忆、无线连接等。在硬件上，这些都不是技术难点。VR、AR的难点都在感知和显示，感知是一种映射，VR 映射的是一个lighthouse的空间或者PS camera mapping的一个交叉；在显示上，VR如何精准地匹配用户的头部产生相应的画面，AR则在这基础上算出光照、遮挡等情况并让图像通透不干扰现实中的视线。而VR硬件的难点在于光学的镜片技术和位置追踪技术（SLAM），因为以前的移动端不涉及这些技术。AR的软件难点在于：1、定位相机；2、恢复场景的三维结构。通常情况下，这一技术被称作SLAM（Simultaneous Localization And Mapping）。当然还有一些其他的技术诸如：图像追踪、云端视觉搜索、人脸和表情追踪等。 目前国内外已经有多家技术公司提供了软件开发方面的AR解决方案和工具，使得全球众多开发者参与到AR应用开发中来。开发者不需要自己搭建系统架构，也不用理解底层SDK复杂的实现方式，只需要将AR模块嵌入到已有的业务逻辑中，就可以通过现成的开源代码或者平台工具，设计并开发属于自己的AR软件产品。就这个层面来讲技术是可以实现的，但就某一个特殊领域的实现方式可能有所差别。 技术支持Metaio 是由德国大众的一个项目衍生出来的一家虚拟现实初创公司，现已被苹果公司收购。 专门从事增强现实和机器视觉解决方案，产品主要包括Metaio SDK 和 Metaio Creator。 Metaio SDK 支持移动设备的AR应用开发，它在内部提供增强现实显示组件ARView，该组件将摄像机层、3D 空间计算以及POI信息的叠加等功能全部封装在一起，用户在使用增强现实功能时，只需要关注用户操作的监听器即可，摄像机层、3D 空间计算、图形识别以及空间信息叠加等逻辑，完全由ARView组件自己处理 。Metaio Creator相对Metaio SDK 来说，使用门槛更低，用户无需掌握移动开发技术，就可以通过 Metaio Creator 用户图形接口中简单的点击、拖拽、拉伸等方式，控制软件中组件的功能，以构建出自己的增强现实结果。（但是被苹果收购了，目前不提供服务）Wikitude 是由美国 Mobilizy 公司于 2008 年秋推出的一款移动增强现实开发平台， 支持 Android、 iOS、Black Berry 以及 Windows Phone 多个手机智能操作系统Wikitude SDK 是一款优秀的增强现实开发工具包， 它能够帮助开发人员减小增强现实应用程序开发的复杂性。 目前，Wikitude SDK 支持载入真实的物理环境向 AR 环境中添加虚拟物体、支持用户与虚拟物体的交互、响应用户的位置变化、AR 环境中信息提示、从本地或网络加载资源等功能。图一Wikitude 官网界面 ENTiTi Creator是由以色列一家创业公司 Waking App 开发的一款 AR 作品制作工具，易学易用是它的最大特色。用户可以使用ENTiTi平台上传图片和视频以及相应的动作指令， 并通过简单的逻辑串联，就可以轻松创建出包含3D图像、动画或者游戏的AR/VR 内容。该平台不需要任何编程、完全依靠鼠标拖放就能完成整个创建过程。EN-TiTi是基于云计算的平台，可以在线 3D 视角查看内容，并自动适配各种终端，比如，手机或平台电脑、三星 Gear VR 盒子、Vuzix 智能眼镜等。开发者通过它所发布出来的AR内容，只需要通过一个叫作 EN-TiTi View 软件的入口，就可以轻松访问。 这意味着全球所有开发者所开发出的成千上万的 AR 内容，只需要一个软件即可全部浏览。 图二ENTiTi Creator 官网介绍 Realmax公司是一个国际化AR生态级企业，在上海、香港、纽约、慕尼黑都设立了分公司，并建立了5个全球实验室，完成了硬件量产、软件算法、应用开发和内容制作的AR技术储备，AR操作系统“Realcast”也有可观的用户量，在工业、幼教、电商、旅游等领域积累了大量客户，是AR领域唯一的一家完成“平台+内容+终端+应用”生态链布局的企业。图三 Realmax公司官网主页 根据以上分析，如果想完成某一个特定的AR或者VR应用，可以使用上述公司提供的SDK，在一定程度上会加快开发速度。 参考文献1． 增强现实公司类型：http://www.marxentlabs.com/augmented-reality-company-primer-5-types-augmented-reality-companies/2． Metaio公司主页：http://www.metaio.eu/3． AR技术举例以及现有公司介绍：http://www.marxentlabs.com/what-is-virtual-reality-definition-and-examples/。4． Layer公司AR开发SDK：https://www.layar.com/solutions/#sdk5． Wikitude官网：http://www.metaio.eu/index.html6． ENTiTi Creator 官网：http://www.wakingapp.com/7． Realmax公司官网：http://www.realmax.com/或者http://www.realmax.com.hk/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Markdown 学习]]></title>
      <url>%2F2016%2F08%2F08%2FMarkdown%E5%AD%A6%E4%B9%A0%2F</url>
      <content type="text"><![CDATA[本文涉及学习Markdown文本标记语言的一些练习笔记。 Note Tag 测试1234567891011121314/** * note.js | global hexo script. * * ATTENTION! No need to write this tag in 1 line if u don't want see probally bugs. * * Usage: * * &#123;% note [class] %&#125; * Any content (support inline tags too). * &#123;% endnote %&#125; * * [class] : default | primary | success | info | warning | danger. * May be not defined. */ Test note default昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note primary昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note success昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note info昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note warning昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note danger昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Button 标签测试12Usage: &#123;% button /path/to/url/, text, icon [class], title %&#125;Alias: &#123;% btn /path/to/url/, text, icon [class], title %&#125; Button内嵌文字1&#123;% button #, Text %&#125;&#123;% button #插入不同颜色的字体, 插入不同颜色的字体,heart %&#125; Text插入不同颜色的字体 Button内嵌logo123&lt;div class="text-center"&gt;&lt;span&gt;&#123;% btn ##插入不同颜色的字体,, header %&#125;&#123;% btn #,, edge %&#125;&#123;% btn #,, times %&#125;&#123;% btn #,, circle-o %&#125;&lt;/span&gt;&lt;span&gt;&#123;% btn #,, italic %&#125;&#123;% btn #,, scribd %&#125;&lt;/span&gt;&lt;span&gt;&#123;% btn #,, google %&#125;&#123;% btn #,, chrome %&#125;&#123;% btn #,, opera %&#125;&#123;% btn #,, diamond fa-rotate-270 %&#125;&lt;/span&gt;&lt;/div&gt; 插入不同颜色的字体 Button Margin1&lt;div class="text-center"&gt;&#123;% btn #, Almost, adn fa-fw fa-lg %&#125; &#123;% btn #, Over, terminal fa-fw fa-lg %&#125;&lt;/div&gt; Almost Over 123&lt;div class="text-right"&gt;&#123;% btn #, Test is finished., check fa-fw fa-lg, Button tag test is finished. %&#125;&lt;/div&gt; Test is finished. Label Tag测试文中字体颜色1234567891011121314From &#123;% label @fairest creatures %&#125; we desire increase,That thereby &#123;% label primary@beauty's rose %&#125; might never die,But as the &#123;% label success@riper %&#125; should by time decease,His tender heir might &#123;% label info@bear his memory %&#125;:But thou contracted to thine own bright eyes,Feed'st thy light's flame with *&#123;% label warning @self-substantial fuel%&#125;*,Making a famine where ~~&#123;% label default @abundance lies %&#125;~~,Thy self thy foe, to thy &lt;mark&gt;sweet self too cruel&lt;/mark&gt;:Thou that art now the world's fresh ornament,And only herald to the gaudy spring,Within thine own bud buriest thy content,And &#123;% label danger@tender churl mak'st waste in niggarding %&#125;:Pity the world, or else this glutton be,&#123;% label warning@To eat the world's due, by the grave and thee %&#125;. From fairest creatures we desire increase,That thereby beauty's rose might never die,But as the riper should by time decease,His tender heir might bear his memory:But thou contracted to thine own bright eyes,Feed’st thy light’s flame with self-substantial fuel,Making a famine where abundance lies,Thy self thy foe, to thy sweet self too cruel:Thou that art now the world’s fresh ornament,And only herald to the gaudy spring,Within thine own bud buriest thy content,And tender churl mak'st waste in niggarding:Pity the world, or else this glutton be,To eat the world's due, by the grave and thee. 表格Tag测试1234567891011&#123;% tabs First unique name %&#125;&lt;!-- tab --&gt;**This is Tab 1.**&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**This is Tab 2.**&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**This is Tab 3.**&lt;!-- endtab --&gt;&#123;% endtabs %&#125; First unique name 1First unique name 2First unique name 3This is Tab 1. This is Tab 2. This is Tab 3. 插入不同颜色的字体1&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=LimeGreen&gt;&lt;font color=white size=3&gt;我是白色的字体，背景是色的~&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 我是白色的字体，背景是色的~ 我是白色的字体，背景是深灰色的~ 我是白色的字体，背景是浅海绿的~ 我是白色的字体，背景是蓝色的~ 我是白色的字体，背景是银色的~ 我是白色的字体，背景是淡灰色的~ 我是白色的字体，背景是深灰色的~ 插入代码这里是代码区域 1234567891011121314151617181920212223242526% The following is the Matlab Code% I want to see the resultfunction demo()temp=zeros(5,6);for i=1:size(temp,1) for j=1:size(temp,2) temp(i,j)=rand(1); if temp(i,j)&gt;0.5 temp(i,j)=1; end endendreturn temp 插入标题1234567891011# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 列表123456- 文本1- 文本2- 文本31. 文本一2. 文本二3. 文本三 文本1 文本2 文本3 文本一 文本二 文本三 插入图像1![](http://pic1.win4000.com/wallpaper/0/54cae8e69ac8b.jpg) 或者：1&lt;center&gt;&lt;img src="http://i2.wp.com/posturemag.com/online/wp-content/uploads/2015/07/Kaz7.jpg" width="100%" &gt;&lt;/center&gt; 插入链接 segmentfault上的一个Markdown学习手册 有道云笔记的Markdown学习指南-基础篇 Git学习手册 插入公式1$$E=mc^2$$ E=mc^2Hexo文档使用Markdown语言对文档进行编辑，Hexo自身对公式可以进行渲染但是效果不佳，我们采用的是mathjax对Markdown中的公式进行渲染。首先修复Hexo与mathjax之间的渲染冲突，然后可以参考mathjax的说明文档编辑公式。 参考 Hexo Theme Next Test Color map 一个关于Latex不短的介绍 Latex常用命令摘录]]></content>
    </entry>

    
  
  
</search>
